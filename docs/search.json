[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Peter Atkins",
    "section": "",
    "text": "Welcome to my website! I am an Operations Manager at MD Revolution and an MBA Candidate at UCSD Rady School of Management. I am passionate about healthcare, finance, supply chain, technology, and entrepreneurship. I am excited to share my journey with you. Please feel free to reach out to me if you have any questions or would like to connect."
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Peter Atkins CV",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nPeter Atkins\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMNL & Conjoint Analysis\n\n\n\n\n\n\nPeter Atkins\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKey Drivers Analysis\n\n\n\n\n\n\nPeter Atkins\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nPeter Atkins\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\nThe data for this project is available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse. The data is in Stata format, so I will use the haven package in R to read the data. The data is in the file karlan_list_2007.dta.\n\nDescription\ntodo: Read the data into R/Python and describe the data\nThis Data is from a field experiment conducted by Dean Karlan at Yale and John List at the University of Chicago to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. The data includes information on the treatment assignment, the response rate, the donation amount, and other variables. The data is in Stata format, so I will use the read_stata function from the pandas package to read the data into a pandas DataFrame. I had to truncate the data to remove missing values. The final dataset has 46,513 observations.\n\n\nTotal rows: 50083\n       treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0              0        1  Control       0       0   Control       0       0   \n2              1        0        1       0       0  $100,000       0       0   \n3              1        0        1       0       0  Unstated       0       0   \n4              1        0        1       0       0   $50,000       0       1   \n5              0        1  Control       0       0   Control       0       0   \n...          ...      ...      ...     ...     ...       ...     ...     ...   \n50078          1        0        1       0       0   $25,000       1       0   \n50079          0        1  Control       0       0   Control       0       0   \n50080          0        1  Control       0       0   Control       0       0   \n50081          1        0        3       0       1  Unstated       0       0   \n50082          1        0        3       0       1   $25,000       1       0   \n\n       size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0            0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n2            1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3            0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4            0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n5            0       0  ...    1.0      0.0  0.862053  0.071572   0.363239   \n...        ...     ...  ...    ...      ...       ...       ...        ...   \n50078        0       0  ...    0.0      1.0  0.872797  0.089959   0.257265   \n50079        0       0  ...    0.0      1.0  0.688262  0.108889   0.288792   \n50080        0       0  ...    1.0      0.0  0.900000  0.021311   0.178689   \n50081        0       1  ...    1.0      0.0  0.917206  0.008257   0.225619   \n50082        0       0  ...    0.0      1.0  0.530023  0.074112   0.340698   \n\n       ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0           2.10          28517.0  0.499807      0.324528       1.000000  \n2           2.48          51175.0  0.721941      0.192668       1.000000  \n3           2.65          79269.0  0.920431      0.412142       1.000000  \n4           1.85          40908.0  0.416072      0.439965       1.000000  \n5           2.92          61779.0  0.941339      0.200840       0.962345  \n...          ...              ...       ...           ...            ...  \n50078       2.13          45047.0  0.771316      0.263744       1.000000  \n50079       2.67          74655.0  0.741931      0.586466       1.000000  \n50080       2.36          26667.0  0.778689      0.107930       0.000000  \n50081       2.57          39530.0  0.733988      0.184768       0.634903  \n50082       3.70          48744.0  0.717843      0.127941       0.994181  \n\n[46513 rows x 51 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\n\n\nT-Test and Linear Regression\n\n\nt-test results: t-statistic = 0.062169999954557034 , p-value = 0.9504277199691156\nLinear regression results:\n                             OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                  0.003865\nDate:                Sat, 13 Apr 2024   Prob (F-statistic):              0.950\nTime:                        15:52:11   Log-Likelihood:            -1.8175e+05\nNo. Observations:               46513   AIC:                         3.635e+05\nDf Residuals:                   46511   BIC:                         3.635e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9465      0.097    133.797      0.000      12.757      13.136\ntreatment      0.0074      0.118      0.062      0.950      -0.225       0.240\n==============================================================================\nOmnibus:                     7579.664   Durbin-Watson:                   1.999\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            11864.002\nSkew:                           1.170   Prob(JB):                         0.00\nKurtosis:                       3.802   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\nIn the T-Test, my t-statistic = .0621, and my pvalue = .950. In the linear regression, the coefficient on the treatment variable is .0074, and the p-value is .950. Both results are the same, and they are not statistically significant at the 95% confidence level. This suggests that the treatment and control groups are balanced on the variable mrm2. The treatment has a negligible effect. This is consistent with the results in Table 1 of the paper, which shows that the treatment and control groups are balanced on all variables.\nt-test results: t-statistic = 0.062169999954557034 , p-value = 0.9504277199691156\nLinear regression results:"
  },
  {
    "objectID": "hw1_questions.html#bar-plot---proportion-of-people-who-donated-by-treatment-group",
    "href": "hw1_questions.html#bar-plot---proportion-of-people-who-donated-by-treatment-group",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Bar Plot - Proportion of People Who Donated by Treatment Group",
    "text": "Bar Plot - Proportion of People Who Donated by Treatment Group\n_todo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made.\n\nT-Test Results - Treatment/Control on Charitable Donation - Bianary\n\n\nt-test results: t-statistic = 3.4520847762481726 , p-value = 0.0005567671909681928\n\n\nt-test results: t-statistic = 3.452084 , p-value = 0.000556\nAlso run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.)\n\n\nBivarariate linear regression - Treatment/Control on Charitable Donation\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     11.92\nDate:                Sat, 13 Apr 2024   Prob (F-statistic):           0.000557\nTime:                        15:52:12   Log-Likelihood:                 24504.\nNo. Observations:               46513   AIC:                        -4.900e+04\nDf Residuals:                   46511   BIC:                        -4.899e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0176      0.001     15.349      0.000       0.015       0.020\ntreatment      0.0049      0.001      3.452      0.001       0.002       0.008\n==============================================================================\nOmnibus:                    55348.369   Durbin-Watson:                   2.001\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3923594.986\nSkew:                           6.704   Prob(JB):                         0.00\nKurtosis:                      45.951   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader (Likert Plot, Binary Plot, Continious Plot). To do this, you will simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of Results\nthe cumulative average difference in donation amounts approaches the true difference in means as the number of draws increases. This is consistent with the Law of Large Numbers, which states that the sample average approaches the true population average as the sample size increases.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of Results\nthe larger the sample size, the more the distribution of average differences in donation amounts approaches a normal distribution. This is consistent with the Central Limit Theorem, which states that the distribution of sample averages approaches a normal distribution as the sample size increases. The distribution of average differences in donation amounts is centered around zero, indicating that the treatment and control groups have similar average donation amounts. the zero is slightly skewed towards the right. This suggests that the treatment group donated slightly more on average than the control group."
  },
  {
    "objectID": "projects/Project1/hw1_questions.html",
    "href": "projects/Project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/Project1/hw1_questions.html#introduction",
    "href": "projects/Project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/Project1/hw1_questions.html#data",
    "href": "projects/Project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\nThe data for this project is available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse. The data is in Stata format, so I will use the haven package in R to read the data. The data is in the file karlan_list_2007.dta.\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\n\n# Path to the Stata file\nfile_path = \"/Users/peteratkins/Downloads/karlan_list_2007.dta\"\n\n# Read the Stata file into a pandas DataFrame\ndf = pd.read_stata(file_path)\ndf_dropna = df.dropna()\n\n\n# Print the total number of rows\nprint(\"Total rows:\", len(df))\n\nTotal rows: 50083\n\n\n\n\nData\n\n\n# Print all rows\nprint(df_dropna)\n\n       treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0              0        1  Control       0       0   Control       0       0   \n2              1        0        1       0       0  $100,000       0       0   \n3              1        0        1       0       0  Unstated       0       0   \n4              1        0        1       0       0   $50,000       0       1   \n5              0        1  Control       0       0   Control       0       0   \n...          ...      ...      ...     ...     ...       ...     ...     ...   \n50078          1        0        1       0       0   $25,000       1       0   \n50079          0        1  Control       0       0   Control       0       0   \n50080          0        1  Control       0       0   Control       0       0   \n50081          1        0        3       0       1  Unstated       0       0   \n50082          1        0        3       0       1   $25,000       1       0   \n\n       size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0            0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n2            1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3            0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4            0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n5            0       0  ...    1.0      0.0  0.862053  0.071572   0.363239   \n...        ...     ...  ...    ...      ...       ...       ...        ...   \n50078        0       0  ...    0.0      1.0  0.872797  0.089959   0.257265   \n50079        0       0  ...    0.0      1.0  0.688262  0.108889   0.288792   \n50080        0       0  ...    1.0      0.0  0.900000  0.021311   0.178689   \n50081        0       1  ...    1.0      0.0  0.917206  0.008257   0.225619   \n50082        0       0  ...    0.0      1.0  0.530023  0.074112   0.340698   \n\n       ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0           2.10          28517.0  0.499807      0.324528       1.000000  \n2           2.48          51175.0  0.721941      0.192668       1.000000  \n3           2.65          79269.0  0.920431      0.412142       1.000000  \n4           1.85          40908.0  0.416072      0.439965       1.000000  \n5           2.92          61779.0  0.941339      0.200840       0.962345  \n...          ...              ...       ...           ...            ...  \n50078       2.13          45047.0  0.771316      0.263744       1.000000  \n50079       2.67          74655.0  0.741931      0.586466       1.000000  \n50080       2.36          26667.0  0.778689      0.107930       0.000000  \n50081       2.57          39530.0  0.733988      0.184768       0.634903  \n50082       3.70          48744.0  0.717843      0.127941       0.994181  \n\n[46513 rows x 51 columns]"
  },
  {
    "objectID": "projects/Project1/hw1_questions.html#experimental-results",
    "href": "projects/Project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\nIn the T-Test, my t-statistic = .0621, and my pvalue = .950. In the linear regression, the coefficient on the treatment variable is .0074, and the p-value is .950. Both results are the same, and they are not statistically significant at the 95% confidence level. This suggests that the treatment and control groups are balanced on the variable mrm2. The treatment has a negligible effect. This is consistent with the results in Table 1 of the paper, which shows that the treatment and control groups are balanced on all variables.\nt-test results: t-statistic = 0.062169999954557034 , p-value = 0.9504277199691156\nLinear regression results:"
  },
  {
    "objectID": "projects/Project1/hw1_questions.html#bar-plot---proportion-of-people-who-donated-by-treatment-group",
    "href": "projects/Project1/hw1_questions.html#bar-plot---proportion-of-people-who-donated-by-treatment-group",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Bar Plot - Proportion of People Who Donated by Treatment Group",
    "text": "Bar Plot - Proportion of People Who Donated by Treatment Group\n_todo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made.\n\nT-Test Results - Treatment/Control on Charitable Donation - Bianary\n\n\nt-test results: t-statistic = 3.4520847762481726 , p-value = 0.000556767190968193\n\n\nt-test results: t-statistic = 3.452084 , p-value = 0.000556\nAlso run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.)\n\n\nBivarariate linear regression - Treatment/Control on Charitable Donation\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     11.92\nDate:                Sun, 14 Apr 2024   Prob (F-statistic):           0.000557\nTime:                        10:00:30   Log-Likelihood:                 24504.\nNo. Observations:               46513   AIC:                        -4.900e+04\nDf Residuals:                   46511   BIC:                        -4.899e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0176      0.001     15.349      0.000       0.015       0.020\ntreatment      0.0049      0.001      3.452      0.001       0.002       0.008\n==============================================================================\nOmnibus:                    55348.369   Durbin-Watson:                   2.001\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3923594.986\nSkew:                           6.704   Prob(JB):                         0.00\nKurtosis:                      45.951   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "projects/Project1/hw1_questions.html#simulation-experiment",
    "href": "projects/Project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\nCode\n\n\n# Simulate 10,000 draws from the control and treatment distributions\ndist_control = np.random.normal(loc=avg_donation_control, scale=np.std(donation_amount_control), size=10000)\ndist_treatment = np.random.normal(loc=avg_donation_treatment, scale=np.std(donation_amount_treatment), size=10000)\n\n#diference calc\ndifferences = dist_treatment - dist_control\n\n#cumulative average\ncum_avg = np.cumsum(differences) / np.arange(1, 10001)\n\n#plot\nplt.figure(figsize=(12, 6))\nplt.plot(cum_avg, color='blue')\nplt.axhline(y=(avg_donation_treatment - avg_donation_control), color='red', linestyle='dashed', linewidth=1)\nplt.xlabel('Number of Draws')\nplt.ylabel('Cumulative Average Difference')\nplt.title('Cumulative Average Difference in Donation Amounts')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe cumulative average difference in donation amounts approaches the true difference in means as the number of draws increases. This is consistent with the Law of Large Numbers, which states that the sample average approaches the true population average as the sample size increases.\n\n\nCentral Limit Theorem\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\nfile_path = \"/Users/peteratkins/Downloads/karlan_list_2007.dta\"\ndf = pd.read_stata(file_path)\ndf_dropna = df.dropna()\n\ndf_donors = df_dropna[df_dropna['amount'] &gt; 0]\ndonation_amount_control = df_donors[df_donors['control'] == 1]['amount']\ndonation_amount_treatment = df_donors[df_donors['treatment'] == 1]['amount']\n\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axs = plt.subplots(len(sample_sizes), 1, figsize=(12, 24))\n\nfor i, sample_size in enumerate(sample_sizes):\n    averages = []\n    for _ in range(1000):\n        control_sample = np.random.choice(donation_amount_control, size=sample_size)\n        treatment_sample = np.random.choice(donation_amount_treatment, size=sample_size)\n        averages.append(np.mean(treatment_sample) - np.mean(control_sample))\n    axs[i].hist(averages, bins=30, color='blue', alpha=0.7)\n    axs[i].axvline(0, color='red', linestyle='dashed', linewidth=1)\n    axs[i].set_title(f'Sample Size: {sample_size}')\n    axs[i].set_xlabel('Average Difference in Donation Amounts')\n    axs[i].set_ylabel('Frequency')\n\n    mu, std = norm.fit(averages)\n    xmin, xmax = axs[i].get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    axs[i].plot(x, p, 'r', linewidth=2)\n\n    # Formatting\n    axs[i].set_title(f'Sample Size: {sample_size}')\n    axs[i].set_xlabel('Average Difference in Donation Amounts')\n    axs[i].set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe larger the sample size, the more the distribution of average differences in donation amounts approaches a normal distribution. This is consistent with the Central Limit Theorem, which states that the distribution of sample averages approaches a normal distribution as the sample size increases. The distribution of average differences in donation amounts is centered around zero, indicating that the treatment and control groups have similar average donation amounts. The zero is slightly skewed towards the right. This suggests that the treatment group donated slightly more on average than the control group."
  },
  {
    "objectID": "patkins64.github.io/index.html",
    "href": "patkins64.github.io/index.html",
    "title": "Peter Atkins",
    "section": "",
    "text": "Welcome to my website! I am an Operations Manager at MD Revolution and an MBA Candidate at UCSD Rady School of Management. I am passionate about healthcare, finance, supply chain, technology, and entrepreneurship. I am excited to share my journey with you. Please feel free to reach out to me if you have any questions or would like to connect."
  },
  {
    "objectID": "patkins64.github.io/projects.html",
    "href": "patkins64.github.io/projects.html",
    "title": "My Projects",
    "section": "",
    "text": "My Main Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle: Second Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "patkins64.github.io/CV.html",
    "href": "patkins64.github.io/CV.html",
    "title": "Peter Atkins CV",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/Project1/hw1_questions.html#description",
    "href": "projects/Project1/hw1_questions.html#description",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Description",
    "text": "Description\nThis Data is from a field experiment conducted by Dean Karlan at Yale and John List at the University of Chicago to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. The data includes information on the treatment assignment, the response rate, the donation amount, and other variables. The data is in Stata format, so I will use the read_stata function from the pandas package to read the data into a pandas DataFrame. I had to truncate the data to remove missing values. The final dataset has 46,513 observations.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nT-Test and Linear Regression\n\n\nCode\n\n\n# Separate the data into treatment and control groups\ntreatment_group = df_dropna[df_dropna['treatment'] == 1]['mrm2']\ncontrol_group = df_dropna[df_dropna['treatment'] == 0]['mrm2']\n\n# Perform a t-test\nt_stat, p_value = stats.ttest_ind(treatment_group, control_group)\nprint(\"t-test results: t-statistic =\", t_stat, \", p-value =\", p_value)\n\n# Perform a linear regression\nX = sm.add_constant(df_dropna['treatment'])  \nY = df_dropna['mrm2']\nmodel = sm.OLS(Y, X)\nresults = model.fit()\nprint(\"Linear regression results:\\n\", results.summary())\n\nt-test results: t-statistic = 0.062169999954557034 , p-value = 0.9504277199691156\nLinear regression results:\n                             OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                  0.003865\nDate:                Sat, 11 May 2024   Prob (F-statistic):              0.950\nTime:                        10:39:07   Log-Likelihood:            -1.8175e+05\nNo. Observations:               46513   AIC:                         3.635e+05\nDf Residuals:                   46511   BIC:                         3.635e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9465      0.097    133.797      0.000      12.757      13.136\ntreatment      0.0074      0.118      0.062      0.950      -0.225       0.240\n==============================================================================\nOmnibus:                     7579.664   Durbin-Watson:                   1.999\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            11864.002\nSkew:                           1.170   Prob(JB):                         0.00\nKurtosis:                       3.802   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nIn the T-Test, my t-statistic = .0621, and my pvalue = .950. In the linear regression, the coefficient on the treatment variable is .0074, and the p-value is .950. Both results are the same, and they are not statistically significant at the 95% confidence level. This suggests that the treatment and control groups are balanced on the variable mrm2. The treatment has a negligible effect. This is consistent with the results in Table 1 of the paper, which shows that the treatment and control groups are balanced on all variables.\nt-test results: t-statistic = 0.062169999954557034 , p-value = 0.9504277199691156\n\n\nLinear regression results:\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                  0.003865\nDate:                Sat, 11 May 2024   Prob (F-statistic):              0.950\nTime:                        10:39:07   Log-Likelihood:            -1.8175e+05\nNo. Observations:               46513   AIC:                         3.635e+05\nDf Residuals:                   46511   BIC:                         3.635e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9465      0.097    133.797      0.000      12.757      13.136\ntreatment      0.0074      0.118      0.062      0.950      -0.225       0.240\n==============================================================================\nOmnibus:                     7579.664   Durbin-Watson:                   1.999\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            11864.002\nSkew:                           1.170   Prob(JB):                         0.00\nKurtosis:                       3.802   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nCode\n\n\ntreatment_donated = df_dropna[(df_dropna['treatment'] == 1) & (df_dropna['gave'] == 1)].shape[0]\nprint(treatment_donated)\n\ntreatment_total = df_dropna[df_dropna['treatment'] == 1].shape[0]\nprint(treatment_total)\n\ntreatment_donation_rate = treatment_donated / treatment_total\nprint(\"Treatment Donation Rate: {:.2%}\".format(treatment_donation_rate))\n\n\ncontrol_donated = df_dropna[(df_dropna['treatment'] == 0) & (df_dropna['gave'] == 1)].shape[0]\nprint(control_donated)\n\ncontrol_total = df_dropna[df_dropna['treatment'] == 0].shape[0]\nprint(control_total)\n\ncontrol_donation_rate = control_donated / treatment_total\nprint(\"Control Donation Rate: {:.2%}\".format(control_donation_rate))\n\n\n# Create a barplot\nplt.bar(['Treatment', 'Control'], [treatment_donation_rate, control_donation_rate])\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of People Who Donated by Treatment Group')\nplt.show()\n\n697\n31018\nTreatment Donation Rate: 2.25%\n273\n15495\nControl Donation Rate: 0.88%\n\n\n\n\n\n\n\n\n\n\n\nBar Plot - Proportion of People Who Donated by Treatment Group\n\n\n\n\n\n\n\n\n\n\n\nT-Test Results - Treatment/Control on Charitable Donation - Bianary\n\n\nCode\n\n\ntreatment_group2 = df_dropna[df_dropna['treatment'] == 1]['gave']\ncontrol_group2 = df_dropna[df_dropna['treatment'] == 0]['gave']\n\nt_stat, p_value = stats.ttest_ind(treatment_group2, control_group2)\n\n\n\n\n\n\nt-test results: t-statistic = 3.4520847762481726 , p-value = 0.0005567671909681928\n\n\n\n\nBivarariate linear regression - Treatment/Control on Charitable Donation\n\n\nCode\n\n\n# Perform a linear regression on Treatment\ntreatmentdonation = 'gave ~ treatment'\n\n# fit the model\ntreatmentdonationmodel = smf.ols(treatmentdonation, data=df_dropna).fit()\n\n\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     11.92\nDate:                Sat, 11 May 2024   Prob (F-statistic):           0.000557\nTime:                        10:39:07   Log-Likelihood:                 24504.\nNo. Observations:               46513   AIC:                        -4.900e+04\nDf Residuals:                   46511   BIC:                        -4.899e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0176      0.001     15.349      0.000       0.015       0.020\ntreatment      0.0049      0.001      3.452      0.001       0.002       0.008\n==============================================================================\nOmnibus:                    55348.369   Durbin-Watson:                   2.001\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3923594.986\nSkew:                           6.704   Prob(JB):                         0.00\nKurtosis:                      45.951   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nThe P-Value of &lt; .05 indicates that the treatment group had a statistically significant higher rate of charitable donations than the control group. This suggests that the treatment group was more likely to donate than the control group. This is consistent with the results in Table 2a Panel A of the paper, which shows that the treatment group had a higher rate of charitable donations than the control group.\n\n\nCode\n\n\n### Defining Variables\nY = df_dropna['gave']\nX = sm.add_constant(df_dropna['treatment'])\n\n#fit the model\nprobit_model = sm.Probit(Y, X).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.101214\n         Iterations 7\n\n\n\n\nRegression - Treatment on Charitable Donation\n\n\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                46513\nModel:                         Probit   Df Residuals:                    46511\nMethod:                           MLE   Df Model:                            1\nDate:                Sat, 11 May 2024   Pseudo R-squ.:                0.001300\nTime:                        10:39:07   Log-Likelihood:                -4707.8\nconverged:                       True   LL-Null:                       -4713.9\nCovariance Type:            nonrobust   LLR p-value:                 0.0004642\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1056      0.024    -86.598      0.000      -2.153      -2.058\ntreatment      0.1004      0.029      3.466      0.001       0.044       0.157\n==============================================================================\n\n\n\n\n\nMy treatment coefficient of .1004 is statistically significant at the 95% confidence level. This is consistent with the results in Table 3 column 1 of the paper, which shows that the treatment coefficient is statistically significant at the 99% confidence level.\n\n\nT-Test Results - 1:1 Match and 2:1 Match\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\nCode\n\n\nfrom scipy.stats import ttest_ind\n\n# donations for 1:1 match and 2:1 match\ndonations_ratio1 = df_dropna[df_dropna['ratio'] ==1]['gave']\ndonations_ratio2 = df_dropna[df_dropna['ratio'] ==2]['gave']\n\n# T-test\nt_stat, p_value = ttest_ind(donations_ratio1, donations_ratio2)\n\n\n\n\n\n\nT-stat: -1.0695278671529307\nP-value: 0.28484437587317657\n\n\n\n\nRegressing on Ratios\n\n\nCode\n\n\n# Create a new variable for the ratio\ndf_dropna['ratio1'] = (df_dropna['ratio'] == 1).astype(int)\n\n## dep var\nY = df_dropna['gave']\n\n## indep var\nX = sm.add_constant(df_dropna[['ratio1', 'ratio2', 'ratio3']])\n\n#fit the model\n\nratio_model = sm.OLS(Y,X).fit()\n\n/var/folders/n1/t0q_fh6x5ng4n3nhdjhlmbgw0000gp/T/ipykernel_24187/1727008199.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     4.671\nDate:                Sat, 11 May 2024   Prob (F-statistic):            0.00289\nTime:                        10:39:07   Log-Likelihood:                 24505.\nNo. Observations:               46513   AIC:                        -4.900e+04\nDf Residuals:                   46509   BIC:                        -4.897e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0176      0.001     15.349      0.000       0.015       0.020\nratio1         0.0032      0.002      1.778      0.075      -0.000       0.007\nratio2         0.0054      0.002      2.974      0.003       0.002       0.009\nratio3         0.0059      0.002      3.273      0.001       0.002       0.009\n==============================================================================\nOmnibus:                    55346.049   Durbin-Watson:                   2.001\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          3922911.266\nSkew:                           6.703   Prob(JB):                         0.00\nKurtosis:                      45.947   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nThe P value for the three ratios are all less than .05, suggesting that the 2:1 and 3:1 match ratios lead to a statistically significant increase in the likelihood that someone donates compared to the 1:1 match ratio. This is consistent with the results in Table 3 column 2 of the paper, which shows that the 2:1 and 3:1 match ratios lead to a statistically significant increase in the likelihood that someone donates compared to the 1:1 match ratio.\n\n\nCode\n\n\n\nResponse Rate Difference between 1:1 and 2:1 Match Ratios: -0.0021795099373574135\nResponse Rate Difference between 2:1 and 3:1 Match Ratios: -0.000533513218346264\nCoefficient Difference between 1:1 and 2:1 Match Ratios: -0.0021795099373575115\nCoefficient Difference between 2:1 and 3:1 Match Ratios: -0.0005335132183461435\n\n\n\n\nResponse Rate and Coefficient Difference between Ratios\n\n\nResponse Rate Difference between 1:1 and 2:1 Match Ratios: -0.0021795099373574135\nResponse Rate Difference between 2:1 and 3:1 Match Ratios: -0.000533513218346264\nCoefficient Difference between 1:1 and 2:1 Match Ratios: -0.0021795099373575115\nCoefficient Difference between 2:1 and 3:1 Match Ratios: -0.0005335132183461435\n\n\n\n\nTreatment effect on Size of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\nCode\n\n\ndonation_amount_treatment = df_dropna[df_dropna['treatment'] == 1]['amount']\ndonation_amount_control = df_dropna[df_dropna['control'] == 1]['amount']\n#T-test\nt_stat, p_value = ttest_ind(donation_amount_treatment, donation_amount_control)\n\n\n\n\n\n\nT-stat: 2.1167886253100887\nP-value: 0.03428308609067127\n\n\nThe T-stat of 2.1167 and P-value of .0343 indicates that the treatment group had a statistically significant higher average donation amount than the control group. This suggests that the treatment group donated more on average than the control group.\n\n\nCode\n\n\n#Filter out people who did not donate\ndf_donors = df_dropna[df_dropna['amount'] &gt; 0]\n\n#dep var   \nY = df_donors['amount']\n\n#indep var\nX = sm.add_constant(df_donors['treatment'])\n\n#fit the model\nfiltered_model = sm.OLS(Y,X).fit()\n\nprint(filtered_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3300\nDate:                Sat, 11 May 2024   Prob (F-statistic):              0.566\nTime:                        10:39:07   Log-Likelihood:                -5002.1\nNo. Observations:                 970   AIC:                         1.001e+04\nDf Residuals:                     968   BIC:                         1.002e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         45.6813      2.545     17.948      0.000      40.687      50.676\ntreatment     -1.7248      3.003     -0.574      0.566      -7.617       4.167\n==============================================================================\nOmnibus:                      565.494   Durbin-Watson:                   2.035\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5683.123\nSkew:                           2.522   Prob(JB):                         0.00\nKurtosis:                      13.732   Cond. No.                         3.54\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nFiltered Regression Results - Treatment on Charitable Donation Amount - Conditional\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3300\nDate:                Sat, 11 May 2024   Prob (F-statistic):              0.566\nTime:                        10:39:07   Log-Likelihood:                -5002.1\nNo. Observations:                 970   AIC:                         1.001e+04\nDf Residuals:                     968   BIC:                         1.002e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         45.6813      2.545     17.948      0.000      40.687      50.676\ntreatment     -1.7248      3.003     -0.574      0.566      -7.617       4.167\n==============================================================================\nOmnibus:                      565.494   Durbin-Watson:                   2.035\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5683.123\nSkew:                           2.522   Prob(JB):                         0.00\nKurtosis:                      13.732   Cond. No.                         3.54\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe Filtered Model shows that the treatment coefficient is not statistically significant at the 95% confidence level. This suggests that the treatment group did not donate significantly more on average than the control group conditional on donating some positive amount. This is consistent with the results in Table 2a Panel B of the paper, which shows that the treatment coefficient is not statistically significant at the 95% confidence level.\n\n\nHistogram of Charitable Donation Amounts by Treatment Group\n\n\nCode\n\n\ndonation_amount_control = df_donors[df_donors['control'] == 1]['amount']\ndonation_amount_treatment = df_donors[df_donors['treatment'] == 1]['amount']\n\navg_donation_treatment = donation_amount_treatment.mean()\navg_donation_control = donation_amount_control.mean()\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.hist(donation_amount_treatment, bins=30, color='blue', alpha=0.7)\nplt.axvline(avg_donation_treatment, color='red', linestyle='dashed', linewidth=1)\nplt.title('Treatment Group')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nplt.hist(donation_amount_control, bins=30, color='green', alpha=0.7)\nplt.axvline(avg_donation_control, color='red', linestyle='dashed', linewidth=1)\nplt.title('Control Group')\nplt.xlabel('Donation Amount')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/Project2/hw2_questions.html",
    "href": "projects/Project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\n\n\nCode\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\nfrom scipy.optimize import minimize_scalar\n\nairbnb_data = \"/Users/peteratkins/Downloads/airbnb.csv\"\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\n\n\ndf_airbnb = pd.read_csv(airbnb_data)\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\n\n\nprint(df_airbnb)\nprint(df_blueprinty)\n\n       Unnamed: 0        id  days last_scraped  host_since        room_type  \\\n0               1      2515  3130     4/2/2017    9/6/2008     Private room   \n1               2      2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2               3      3647  3050     4/2/2017  11/25/2008     Private room   \n3               4      3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4               5      4611  3012     4/2/2017    1/2/2009     Private room   \n...           ...       ...   ...          ...         ...              ...   \n40623       40624  18008937   266     4/2/2017   7/10/2016  Entire home/apt   \n40624       40625  18009045   366     4/2/2017    4/1/2016     Private room   \n40625       40626  18009065   587     4/2/2017   8/24/2015     Private room   \n40626       40627  18009650   335     4/2/2017    5/2/2016     Private room   \n40627       40628  18009669     1     4/2/2017    4/1/2017  Entire home/apt   \n\n       bathrooms  bedrooms  price  number_of_reviews  \\\n0            1.0       1.0     59                150   \n1            1.0       0.0    230                 20   \n2            1.0       1.0    150                  0   \n3            1.0       1.0     89                116   \n4            NaN       1.0     39                 93   \n...          ...       ...    ...                ...   \n40623        1.5       2.0    150                  0   \n40624        1.0       1.0    125                  0   \n40625        1.0       1.0     80                  0   \n40626        1.0       1.0     69                  0   \n40627        1.0       1.0    115                  0   \n\n       review_scores_cleanliness  review_scores_location  review_scores_value  \\\n0                            9.0                     9.0                  9.0   \n1                            9.0                    10.0                  9.0   \n2                            NaN                     NaN                  NaN   \n3                            9.0                     9.0                  9.0   \n4                            9.0                     8.0                  9.0   \n...                          ...                     ...                  ...   \n40623                        NaN                     NaN                  NaN   \n40624                        NaN                     NaN                  NaN   \n40625                        NaN                     NaN                  NaN   \n40626                        NaN                     NaN                  NaN   \n40627                        NaN                     NaN                  NaN   \n\n      instant_bookable  \n0                    f  \n1                    f  \n2                    f  \n3                    f  \n4                    t  \n...                ...  \n40623                t  \n40624                f  \n40625                t  \n40626                t  \n40627                t  \n\n[40628 rows x 14 columns]\n      customer_id  patents     region   age      iscustomer\n0               1        0    Midwest  32.5  Not a Customer\n1             786        3  Southwest  37.5  Not a Customer\n2             348        4  Northwest  27.0     Is Customer\n3             927        3  Northeast  24.5  Not a Customer\n4             830        3  Southwest  37.0  Not a Customer\n...           ...      ...        ...   ...             ...\n1495         1366        2  Northeast  18.5     Is Customer\n1496          619        3  Southwest  22.5  Not a Customer\n1497          826        4  Southwest  17.0  Not a Customer\n1498          601        3      South  29.0  Not a Customer\n1499          602        1      South  39.0  Not a Customer\n\n[1500 rows x 5 columns]\n\n\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\nmeans = df_blueprinty.groupby('iscustomer')['patents'].mean()\n\nprint(means)\n\nfor status in df_blueprinty['iscustomer'].unique():\n    plt.hist(df_blueprinty['patents'][df_blueprinty['iscustomer'] == status], alpha=0.5, label=status)\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\niscustomer\nIs Customer       4.091371\nNot a Customer    3.623177\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\nIs Customer       4.091371\nNot a Customer    3.623177\nName: patents, dtype: float64\niscustomer\nIs Customer        197\nNot a Customer    1303\nName: customer_id, dtype: int64\n\n\n\n\n\n\n\n\n\nThe mean for the number of patents for 197 customers in this dataset is 4.1, while the mean for the number of patents for 1303 non-customers is 3.6. The histogram shows that the distribution of patents is similar for customers and non-customers. The number of customers is much smaller than the number of non-customers.\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\nmean_age = df_blueprinty.groupby(['iscustomer', 'region'])['age'].mean().reset_index()\n\n\npivot_table = mean_age.pivot(index='region', columns='iscustomer', values='age')\n\nax = pivot_table.plot(kind='bar')\n\n\nplt.ylabel('Mean Age')\nplt.title('Mean Age by Region and Customer Status')\n\nplt.xticks(rotation=0)\n\nax.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.1f'), \n                (p.get_x() + p.get_width() / 3., p.get_height()), \n                ha = 'center', va = 'center', \n                xytext = (0, 5), \n                textcoords = 'offset points')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDespite the region, customers tend to be younger than non-customers. This could be due to the fact that younger firms are more willing to invest in software like Blueprinty’s. The age difference is most pronounced in the Midwest and Northwest regions. The age difference is smallest in the Southwest region.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import poisson\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\n# Lambda = mean of Y\n\nlambda_ = round(df_blueprinty['patents'].mean(),6)\n\n#likelihoods of each record\nlikelihoods = poisson.pmf(df_blueprinty['patents'], lambda_)\n\ntotal_likelihood = np.prod(likelihoods)\n\nprint(f\"Estimated Lambda = {lambda_}\")\nprint(f\"Likelihoods = {likelihoods}\")\nprint(f\"Total Likelihood = {total_likelihood}\")\n\nformula = \"$$L(\\lambda|Y) = \\prod_{i=1}^{n} f(y_i|\\lambda) = \\prod_{i=1}^{n} e^{-\\lambda} \\frac{\\lambda^{y_i}}{y_i!}$$\"\n\nprint(f\"Likelihood for $y\\sim\\text(poisson)(\\lambda)$formula) = {formula}\")\n\nEstimated Lambda = 3.684667\nLikelihoods = [0.02510553 0.20932107 0.19281961 ... 0.19281961 0.20932107 0.09250553]\nTotal Likelihood = 0.0\nLikelihood for $y\\sim   ext(poisson)(\\lambda)$formula) = $$L(\\lambda|Y) = \\prod_{i=1}^{n} f(y_i|\\lambda) = \\prod_{i=1}^{n} e^{-\\lambda} \frac{\\lambda^{y_i}}{y_i!}$$\n\n\n\n\n\n\n\nEstimated Lambda = 3.684667\nLikelihoods = [0.02510553 0.20932107 0.19281961 ... 0.19281961 0.20932107 0.09250553]\nTotal Likelihood = 0.0\nLikelihood for $y\\sim   ext(poisson)(\\lambda)$formula) = $$L(\\lambda|Y) = \\prod_{i=1}^{n} f(y_i|\\lambda) = \\prod_{i=1}^{n} e^{-\\lambda} \frac{\\lambda^{y_i}}{y_i!}$$\n\n\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import poisson\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\n# Lambda = mean of Y\n\nlambda_ = round(df_blueprinty['patents'].mean(),6)\n\nlog_likelihoods = poisson.logpmf(df_blueprinty['patents'], lambda_)\ntotal_log_likelihood = np.sum(log_likelihoods)\nprint(f\"Total Log Likelihood = {total_log_likelihood}\")\n\nTotal Log Likelihood = -3367.6837722351183\n\n\n\n\n\n\n\nTotal Log Likelihood = -3367.6837722351183\n\n\n\n\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import poisson\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\nlambda_ = round(df_blueprinty['patents'].mean(),6)\n\n## Range of lambdas\nlambdas = np.linspace(0, df_blueprinty['patents'].max(), 100)\n\n## log-likelihoods for each lambda\nlog_likelihoods = [np.sum(poisson.logpmf(df_blueprinty['patents'], lambda_)) for lambda_ in lambdas]\n\n# Plot lambda vs log-likelihood\nplt.figure(figsize=(10, 6))\nplt.plot(lambdas, log_likelihoods, marker='o')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood for Different Lambda Values')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import poisson\nfrom scipy.optimize import minimize\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\nlambda_ = round(df_blueprinty['patents'].mean(),6)\n\n## Range of lambdas\nlambdas = np.linspace(0, df_blueprinty['patents'].max(), 100)\n\n## log-likelihoods for each lambda\nlog_likelihoods = [np.sum(poisson.logpmf(df_blueprinty['patents'], lambda_)) for lambda_ in lambdas]\n\n## negative log-likelihood\ndef neg_log_likelihood(lambda_, Y):\n    return -np.sum(poisson.logpmf(Y, lambda_))\n\n## MLE\nmle = minimize(neg_log_likelihood, lambda_, args=(df_blueprinty['patents']))\n\nprint(F\"MLE for Lambda: {mle.x[0]}\")\n\nMLE for Lambda: 3.684666624262132\n\n\n\n\n\n\n\nMLE for Lambda: 3.684666624262132\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import poisson\nfrom scipy.optimize import minimize\n\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\ndf_blueprinty['age2'] = df_blueprinty['age']**2\ndf_blueprinty = pd.get_dummies(df_blueprinty, columns=['region', 'iscustomer'])\nY = df_blueprinty['patents']\nX = df_blueprinty.drop('patents', axis=1)\n\nX_standardized = (X - X.mean()) / X.std()\n\nbeta_initial = np.zeros(X.shape[1])\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    X_dot_beta = np.dot(X, beta)\n    X_dot_beta = np.float64(X_dot_beta)  \n    lambda_ = np.exp(X_dot_beta)\n    log_likelihood = np.sum(poisson.logpmf(Y, lambda_))\n    return -log_likelihood\n\nresult = minimize(poisson_regression_loglikelihood, beta_initial, args=(Y, X_standardized))\n\nprint(f\"Estimated betas: {result.x}\")\n\nEstimated betas: [ 8.94195619e-02  4.90597763e+00 -5.42055137e+00 -6.42395258e-03\n  1.71753332e-02 -4.20575528e-02  2.09752125e-02  3.65900439e-03\n  5.26237707e-02 -5.23871097e-02]\n\n\n\n\n\n\n\nEstimated betas: [ 8.94195619e-02  4.90597763e+00 -5.42055137e+00 -6.42395258e-03\n  1.71753332e-02 -4.20575528e-02  2.09752125e-02  3.65900439e-03\n  5.26237707e-02 -5.23871097e-02]\n\n\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndf_blueprinty = pd.read_csv(\"/Users/peteratkins/Downloads/blueprinty.csv\")\n\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\ndf_blueprinty['age2'] = df_blueprinty['age']**2\n\ndf_blueprinty = pd.get_dummies(df_blueprinty, columns=['region', 'iscustomer'])\n\nfor col in df_blueprinty.select_dtypes(include='bool').columns:\n    df_blueprinty[col] = df_blueprinty[col].astype('int64')\n\nY = df_blueprinty['patents']\nX = df_blueprinty.drop('patents', axis=1)\n\nX = sm.add_constant(X)\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1491\nModel Family:                 Poisson   Df Model:                            8\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.7\nDate:                Sat, 11 May 2024   Deviance:                       2178.4\nTime:                        10:17:57   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1155\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                        -0.2319      0.112     -2.075      0.038      -0.451      -0.013\ncustomer_id                6.797e-05      0.000      0.629      0.529      -0.000       0.000\nage                           0.1446      0.014     10.417      0.000       0.117       0.172\nage2                         -0.0029      0.000    -11.135      0.000      -0.003      -0.002\nregion_Midwest               -0.0521      0.058     -0.904      0.366      -0.165       0.061\nregion_Northeast             -0.0276      0.081     -0.341      0.733      -0.186       0.131\nregion_Northwest             -0.0862      0.045     -1.895      0.058      -0.175       0.003\nregion_South                 -0.0218      0.039     -0.559      0.576      -0.098       0.055\nregion_Southwest             -0.0442      0.044     -1.011      0.312      -0.130       0.041\niscustomer_Is Customer       -0.0566      0.059     -0.967      0.334      -0.171       0.058\niscustomer_Not a Customer    -0.1753      0.060     -2.932      0.003      -0.293      -0.058\n=============================================================================================\n\n\n\n\n\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1491\nModel Family:                 Poisson   Df Model:                            8\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.7\nDate:                Sat, 11 May 2024   Deviance:                       2178.4\nTime:                        10:17:57   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1155\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                        -0.2319      0.112     -2.075      0.038      -0.451      -0.013\ncustomer_id                6.797e-05      0.000      0.629      0.529      -0.000       0.000\nage                           0.1446      0.014     10.417      0.000       0.117       0.172\nage2                         -0.0029      0.000    -11.135      0.000      -0.003      -0.002\nregion_Midwest               -0.0521      0.058     -0.904      0.366      -0.165       0.061\nregion_Northeast             -0.0276      0.081     -0.341      0.733      -0.186       0.131\nregion_Northwest             -0.0862      0.045     -1.895      0.058      -0.175       0.003\nregion_South                 -0.0218      0.039     -0.559      0.576      -0.098       0.055\nregion_Southwest             -0.0442      0.044     -1.011      0.312      -0.130       0.041\niscustomer_Is Customer       -0.0566      0.059     -0.967      0.334      -0.171       0.058\niscustomer_Not a Customer    -0.1753      0.060     -2.932      0.003      -0.293      -0.058\n=============================================================================================\n\n\n\n\n\nAge, age squared, and the region of the firm do not have a significant effect on the number of patents awarded. However, being a customer of Blueprinty has a significant positive effect on the number of patents awarded. The coefficient for the customer variable is 0.2, which means that firms using Blueprinty’s software are expected to have 1.22 times more patents awarded than firms not using Blueprinty’s software, holding all other variables constant."
  },
  {
    "objectID": "projects/Project2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/Project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\n\n\nCode\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\nfrom scipy.optimize import minimize_scalar\n\nairbnb_data = \"/Users/peteratkins/Downloads/airbnb.csv\"\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\n\n\ndf_airbnb = pd.read_csv(airbnb_data)\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\n\n\nprint(df_airbnb)\nprint(df_blueprinty)\n\n       Unnamed: 0        id  days last_scraped  host_since        room_type  \\\n0               1      2515  3130     4/2/2017    9/6/2008     Private room   \n1               2      2595  3127     4/2/2017    9/9/2008  Entire home/apt   \n2               3      3647  3050     4/2/2017  11/25/2008     Private room   \n3               4      3831  3038     4/2/2017   12/7/2008  Entire home/apt   \n4               5      4611  3012     4/2/2017    1/2/2009     Private room   \n...           ...       ...   ...          ...         ...              ...   \n40623       40624  18008937   266     4/2/2017   7/10/2016  Entire home/apt   \n40624       40625  18009045   366     4/2/2017    4/1/2016     Private room   \n40625       40626  18009065   587     4/2/2017   8/24/2015     Private room   \n40626       40627  18009650   335     4/2/2017    5/2/2016     Private room   \n40627       40628  18009669     1     4/2/2017    4/1/2017  Entire home/apt   \n\n       bathrooms  bedrooms  price  number_of_reviews  \\\n0            1.0       1.0     59                150   \n1            1.0       0.0    230                 20   \n2            1.0       1.0    150                  0   \n3            1.0       1.0     89                116   \n4            NaN       1.0     39                 93   \n...          ...       ...    ...                ...   \n40623        1.5       2.0    150                  0   \n40624        1.0       1.0    125                  0   \n40625        1.0       1.0     80                  0   \n40626        1.0       1.0     69                  0   \n40627        1.0       1.0    115                  0   \n\n       review_scores_cleanliness  review_scores_location  review_scores_value  \\\n0                            9.0                     9.0                  9.0   \n1                            9.0                    10.0                  9.0   \n2                            NaN                     NaN                  NaN   \n3                            9.0                     9.0                  9.0   \n4                            9.0                     8.0                  9.0   \n...                          ...                     ...                  ...   \n40623                        NaN                     NaN                  NaN   \n40624                        NaN                     NaN                  NaN   \n40625                        NaN                     NaN                  NaN   \n40626                        NaN                     NaN                  NaN   \n40627                        NaN                     NaN                  NaN   \n\n      instant_bookable  \n0                    f  \n1                    f  \n2                    f  \n3                    f  \n4                    t  \n...                ...  \n40623                t  \n40624                f  \n40625                t  \n40626                t  \n40627                t  \n\n[40628 rows x 14 columns]\n      customer_id  patents     region   age      iscustomer\n0               1        0    Midwest  32.5  Not a Customer\n1             786        3  Southwest  37.5  Not a Customer\n2             348        4  Northwest  27.0     Is Customer\n3             927        3  Northeast  24.5  Not a Customer\n4             830        3  Southwest  37.0  Not a Customer\n...           ...      ...        ...   ...             ...\n1495         1366        2  Northeast  18.5     Is Customer\n1496          619        3  Southwest  22.5  Not a Customer\n1497          826        4  Southwest  17.0  Not a Customer\n1498          601        3      South  29.0  Not a Customer\n1499          602        1      South  39.0  Not a Customer\n\n[1500 rows x 5 columns]\n\n\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\nmeans = df_blueprinty.groupby('iscustomer')['patents'].mean()\n\nprint(means)\n\nfor status in df_blueprinty['iscustomer'].unique():\n    plt.hist(df_blueprinty['patents'][df_blueprinty['iscustomer'] == status], alpha=0.5, label=status)\n\nplt.xlabel('Number of Patents')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\niscustomer\nIs Customer       4.091371\nNot a Customer    3.623177\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\nIs Customer       4.091371\nNot a Customer    3.623177\nName: patents, dtype: float64\niscustomer\nIs Customer        197\nNot a Customer    1303\nName: customer_id, dtype: int64\n\n\n\n\n\n\n\n\n\nThe mean for the number of patents for 197 customers in this dataset is 4.1, while the mean for the number of patents for 1303 non-customers is 3.6. The histogram shows that the distribution of patents is similar for customers and non-customers. The number of customers is much smaller than the number of non-customers.\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\nmean_age = df_blueprinty.groupby(['iscustomer', 'region'])['age'].mean().reset_index()\n\n\npivot_table = mean_age.pivot(index='region', columns='iscustomer', values='age')\n\nax = pivot_table.plot(kind='bar')\n\n\nplt.ylabel('Mean Age')\nplt.title('Mean Age by Region and Customer Status')\n\nplt.xticks(rotation=0)\n\nax.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.1f'), \n                (p.get_x() + p.get_width() / 3., p.get_height()), \n                ha = 'center', va = 'center', \n                xytext = (0, 5), \n                textcoords = 'offset points')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDespite the region, customers tend to be younger than non-customers. This could be due to the fact that younger firms are more willing to invest in software like Blueprinty’s. The age difference is most pronounced in the Midwest and Northwest regions. The age difference is smallest in the Southwest region.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import poisson\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\n# Lambda = mean of Y\n\nlambda_ = round(df_blueprinty['patents'].mean(),6)\n\n#likelihoods of each record\nlikelihoods = poisson.pmf(df_blueprinty['patents'], lambda_)\n\ntotal_likelihood = np.prod(likelihoods)\n\nprint(f\"Estimated Lambda = {lambda_}\")\nprint(f\"Likelihoods = {likelihoods}\")\nprint(f\"Total Likelihood = {total_likelihood}\")\n\nformula = \"$$L(\\lambda|Y) = \\prod_{i=1}^{n} f(y_i|\\lambda) = \\prod_{i=1}^{n} e^{-\\lambda} \\frac{\\lambda^{y_i}}{y_i!}$$\"\n\nprint(f\"Likelihood for $y\\sim\\text(poisson)(\\lambda)$formula) = {formula}\")\n\nEstimated Lambda = 3.684667\nLikelihoods = [0.02510553 0.20932107 0.19281961 ... 0.19281961 0.20932107 0.09250553]\nTotal Likelihood = 0.0\nLikelihood for $y\\sim   ext(poisson)(\\lambda)$formula) = $$L(\\lambda|Y) = \\prod_{i=1}^{n} f(y_i|\\lambda) = \\prod_{i=1}^{n} e^{-\\lambda} \frac{\\lambda^{y_i}}{y_i!}$$\n\n\n\n\n\n\n\nEstimated Lambda = 3.684667\nLikelihoods = [0.02510553 0.20932107 0.19281961 ... 0.19281961 0.20932107 0.09250553]\nTotal Likelihood = 0.0\nLikelihood for $y\\sim   ext(poisson)(\\lambda)$formula) = $$L(\\lambda|Y) = \\prod_{i=1}^{n} f(y_i|\\lambda) = \\prod_{i=1}^{n} e^{-\\lambda} \frac{\\lambda^{y_i}}{y_i!}$$\n\n\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import poisson\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\n# Lambda = mean of Y\n\nlambda_ = round(df_blueprinty['patents'].mean(),6)\n\nlog_likelihoods = poisson.logpmf(df_blueprinty['patents'], lambda_)\ntotal_log_likelihood = np.sum(log_likelihoods)\nprint(f\"Total Log Likelihood = {total_log_likelihood}\")\n\nTotal Log Likelihood = -3367.6837722351183\n\n\n\n\n\n\n\nTotal Log Likelihood = -3367.6837722351183\n\n\n\n\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import poisson\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\nlambda_ = round(df_blueprinty['patents'].mean(),6)\n\n## Range of lambdas\nlambdas = np.linspace(0, df_blueprinty['patents'].max(), 100)\n\n## log-likelihoods for each lambda\nlog_likelihoods = [np.sum(poisson.logpmf(df_blueprinty['patents'], lambda_)) for lambda_ in lambdas]\n\n# Plot lambda vs log-likelihood\nplt.figure(figsize=(10, 6))\nplt.plot(lambdas, log_likelihoods, marker='o')\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood for Different Lambda Values')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import poisson\nfrom scipy.optimize import minimize\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\n\nlambda_ = round(df_blueprinty['patents'].mean(),6)\n\n## Range of lambdas\nlambdas = np.linspace(0, df_blueprinty['patents'].max(), 100)\n\n## log-likelihoods for each lambda\nlog_likelihoods = [np.sum(poisson.logpmf(df_blueprinty['patents'], lambda_)) for lambda_ in lambdas]\n\n## negative log-likelihood\ndef neg_log_likelihood(lambda_, Y):\n    return -np.sum(poisson.logpmf(Y, lambda_))\n\n## MLE\nmle = minimize(neg_log_likelihood, lambda_, args=(df_blueprinty['patents']))\n\nprint(F\"MLE for Lambda: {mle.x[0]}\")\n\nMLE for Lambda: 3.684666624262132\n\n\n\n\n\n\n\nMLE for Lambda: 3.684666624262132\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nCode\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import poisson\nfrom scipy.optimize import minimize\n\nblueprinty_data = \"/Users/peteratkins/Downloads/blueprinty.csv\"\ndf_blueprinty = pd.read_csv(blueprinty_data)\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\ndf_blueprinty['age2'] = df_blueprinty['age']**2\ndf_blueprinty = pd.get_dummies(df_blueprinty, columns=['region', 'iscustomer'])\nY = df_blueprinty['patents']\nX = df_blueprinty.drop('patents', axis=1)\n\nX_standardized = (X - X.mean()) / X.std()\n\nbeta_initial = np.zeros(X.shape[1])\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    X_dot_beta = np.dot(X, beta)\n    X_dot_beta = np.float64(X_dot_beta)  \n    lambda_ = np.exp(X_dot_beta)\n    log_likelihood = np.sum(poisson.logpmf(Y, lambda_))\n    return -log_likelihood\n\nresult = minimize(poisson_regression_loglikelihood, beta_initial, args=(Y, X_standardized))\n\nprint(f\"Estimated betas: {result.x}\")\n\nEstimated betas: [ 8.94195619e-02  4.90597763e+00 -5.42055137e+00 -6.42395258e-03\n  1.71753332e-02 -4.20575528e-02  2.09752125e-02  3.65900439e-03\n  5.26237707e-02 -5.23871097e-02]\n\n\n\n\n\n\n\nEstimated betas: [ 8.94195619e-02  4.90597763e+00 -5.42055137e+00 -6.42395258e-03\n  1.71753332e-02 -4.20575528e-02  2.09752125e-02  3.65900439e-03\n  5.26237707e-02 -5.23871097e-02]\n\n\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ndf_blueprinty = pd.read_csv(\"/Users/peteratkins/Downloads/blueprinty.csv\")\n\ndf_blueprinty['iscustomer'] = df_blueprinty['iscustomer'].map({0: 'Not a Customer', 1: 'Is Customer'})\ndf_blueprinty['age2'] = df_blueprinty['age']**2\n\ndf_blueprinty = pd.get_dummies(df_blueprinty, columns=['region', 'iscustomer'])\n\nfor col in df_blueprinty.select_dtypes(include='bool').columns:\n    df_blueprinty[col] = df_blueprinty[col].astype('int64')\n\nY = df_blueprinty['patents']\nX = df_blueprinty.drop('patents', axis=1)\n\nX = sm.add_constant(X)\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\nprint(poisson_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1491\nModel Family:                 Poisson   Df Model:                            8\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.7\nDate:                Sat, 11 May 2024   Deviance:                       2178.4\nTime:                        10:17:57   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1155\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                        -0.2319      0.112     -2.075      0.038      -0.451      -0.013\ncustomer_id                6.797e-05      0.000      0.629      0.529      -0.000       0.000\nage                           0.1446      0.014     10.417      0.000       0.117       0.172\nage2                         -0.0029      0.000    -11.135      0.000      -0.003      -0.002\nregion_Midwest               -0.0521      0.058     -0.904      0.366      -0.165       0.061\nregion_Northeast             -0.0276      0.081     -0.341      0.733      -0.186       0.131\nregion_Northwest             -0.0862      0.045     -1.895      0.058      -0.175       0.003\nregion_South                 -0.0218      0.039     -0.559      0.576      -0.098       0.055\nregion_Southwest             -0.0442      0.044     -1.011      0.312      -0.130       0.041\niscustomer_Is Customer       -0.0566      0.059     -0.967      0.334      -0.171       0.058\niscustomer_Not a Customer    -0.1753      0.060     -2.932      0.003      -0.293      -0.058\n=============================================================================================\n\n\n\n\n\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                patents   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1491\nModel Family:                 Poisson   Df Model:                            8\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3275.7\nDate:                Sat, 11 May 2024   Deviance:                       2178.4\nTime:                        10:17:57   Pearson chi2:                 2.11e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1155\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                        -0.2319      0.112     -2.075      0.038      -0.451      -0.013\ncustomer_id                6.797e-05      0.000      0.629      0.529      -0.000       0.000\nage                           0.1446      0.014     10.417      0.000       0.117       0.172\nage2                         -0.0029      0.000    -11.135      0.000      -0.003      -0.002\nregion_Midwest               -0.0521      0.058     -0.904      0.366      -0.165       0.061\nregion_Northeast             -0.0276      0.081     -0.341      0.733      -0.186       0.131\nregion_Northwest             -0.0862      0.045     -1.895      0.058      -0.175       0.003\nregion_South                 -0.0218      0.039     -0.559      0.576      -0.098       0.055\nregion_Southwest             -0.0442      0.044     -1.011      0.312      -0.130       0.041\niscustomer_Is Customer       -0.0566      0.059     -0.967      0.334      -0.171       0.058\niscustomer_Not a Customer    -0.1753      0.060     -2.932      0.003      -0.293      -0.058\n=============================================================================================\n\n\n\n\n\nAge, age squared, and the region of the firm do not have a significant effect on the number of patents awarded. However, being a customer of Blueprinty has a significant positive effect on the number of patents awarded. The coefficient for the customer variable is 0.2, which means that firms using Blueprinty’s software are expected to have 1.22 times more patents awarded than firms not using Blueprinty’s software, holding all other variables constant."
  },
  {
    "objectID": "projects/Project2/hw2_questions.html#airbnb-case-study",
    "href": "projects/Project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nSummary of Results\n\n\nCode\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"/Users/peteratkins/Downloads/airbnb.csv\")\ndf_airbnb = df.dropna()\ndf_airbnb['days'] = (pd.to_datetime(df_airbnb['last_scraped']) - pd.to_datetime(df_airbnb['host_since'])).dt.days\ndf_airbnb['instant_bookable'] = df_airbnb['instant_bookable'].map({'t': 1, 'f': 0})\ndf_airbnb['room_type'] = df_airbnb['room_type'].map({'Entire home/apt': 0, 'Private room': 1, 'Shared room': 2})\n\nprint(df_airbnb.describe())\n\n# Plot histograms for each variable\ndf_airbnb[['days', 'bathrooms', 'bedrooms', 'price', 'number_of_reviews', 'review_scores_cleanliness', 'review_scores_location', 'review_scores_value']].hist(bins=30, figsize=(15, 10))\nplt.tight_layout()\nplt.show()\n\nX = df_airbnb.drop(['id', 'last_scraped', 'host_since', 'number_of_reviews'], axis=1)\ny = df_airbnb['number_of_reviews']\n\n# Add a constant to the independent variables matrix\nX = sm.add_constant(X)\n\n# Build the Poisson regression model\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n# Print the summary of the model\nprint(poisson_model.summary())\n\n         Unnamed: 0            id          days     room_type     bathrooms  \\\ncount  30140.000000  3.014000e+04  30140.000000  30140.000000  30140.000000   \nmean   18679.598507  8.978322e+06   1112.048275      0.512641      1.122213   \nstd    11318.576418  5.376960e+06    644.430782      0.552982      0.385031   \nmin        1.000000  2.515000e+03      7.000000      0.000000      0.000000   \n25%     8628.750000  4.276596e+06    584.000000      0.000000      1.000000   \n50%    18238.500000  9.149773e+06   1040.000000      0.000000      1.000000   \n75%    28532.250000  1.391476e+07   1591.000000      1.000000      1.000000   \nmax    40504.000000  1.797369e+07   3317.000000      2.000000      6.000000   \n\n           bedrooms         price  number_of_reviews  \\\ncount  30140.000000  30140.000000       30140.000000   \nmean       1.151460    140.211546          21.168115   \nstd        0.699039    188.437967          32.004711   \nmin        0.000000     10.000000           1.000000   \n25%        1.000000     70.000000           3.000000   \n50%        1.000000    103.000000           8.000000   \n75%        1.000000    169.000000          26.000000   \nmax       10.000000  10000.000000         421.000000   \n\n       review_scores_cleanliness  review_scores_location  review_scores_value  \\\ncount               30140.000000            30140.000000         30140.000000   \nmean                    9.201758                9.415428             9.334041   \nstd                     1.114472                0.843181             0.900595   \nmin                     2.000000                2.000000             2.000000   \n25%                     9.000000                9.000000             9.000000   \n50%                    10.000000               10.000000            10.000000   \n75%                    10.000000               10.000000            10.000000   \nmax                    10.000000               10.000000            10.000000   \n\n       instant_bookable  \ncount      30140.000000  \nmean           0.196151  \nstd            0.397091  \nmin            0.000000  \n25%            0.000000  \n50%            0.000000  \n75%            0.000000  \nmax            1.000000  \n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30140\nModel:                            GLM   Df Residuals:                    30129\nModel Family:                 Poisson   Df Model:                           10\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -3.9147e+05\nDate:                Sat, 11 May 2024   Deviance:                   6.6155e+05\nTime:                        10:17:58   Pearson chi2:                 7.99e+05\nNo. Iterations:                     5   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                         3.5696      0.019    189.768      0.000       3.533       3.606\nUnnamed: 0                -6.587e-05   1.64e-07   -401.503      0.000   -6.62e-05   -6.55e-05\ndays                      -5.921e-05   2.55e-06    -23.180      0.000   -6.42e-05   -5.42e-05\nroom_type                     0.0494      0.003     19.473      0.000       0.044       0.054\nbathrooms                    -0.0712      0.004    -18.541      0.000      -0.079      -0.064\nbedrooms                      0.0668      0.002     31.632      0.000       0.063       0.071\nprice                        -0.0002    1.1e-05    -14.648      0.000      -0.000      -0.000\nreview_scores_cleanliness     0.1346      0.002     83.462      0.000       0.131       0.138\nreview_scores_location       -0.0330      0.002    -19.147      0.000      -0.036      -0.030\nreview_scores_value          -0.0566      0.002    -27.877      0.000      -0.061      -0.053\ninstant_bookable              0.6482      0.003    220.862      0.000       0.642       0.654\n=============================================================================================\n\n\n/var/folders/n1/t0q_fh6x5ng4n3nhdjhlmbgw0000gp/T/ipykernel_10637/773579580.py:8: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/n1/t0q_fh6x5ng4n3nhdjhlmbgw0000gp/T/ipykernel_10637/773579580.py:9: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/n1/t0q_fh6x5ng4n3nhdjhlmbgw0000gp/T/ipykernel_10637/773579580.py:10: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:      number_of_reviews   No. Observations:                30140\nModel:                            GLM   Df Residuals:                    30129\nModel Family:                 Poisson   Df Model:                           10\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:            -3.9147e+05\nDate:                Sat, 11 May 2024   Deviance:                   6.6155e+05\nTime:                        10:17:59   Pearson chi2:                 7.99e+05\nNo. Iterations:                     5   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n=============================================================================================\n                                coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nconst                         3.5696      0.019    189.768      0.000       3.533       3.606\nUnnamed: 0                -6.587e-05   1.64e-07   -401.503      0.000   -6.62e-05   -6.55e-05\ndays                      -5.921e-05   2.55e-06    -23.180      0.000   -6.42e-05   -5.42e-05\nroom_type                     0.0494      0.003     19.473      0.000       0.044       0.054\nbathrooms                    -0.0712      0.004    -18.541      0.000      -0.079      -0.064\nbedrooms                      0.0668      0.002     31.632      0.000       0.063       0.071\nprice                        -0.0002    1.1e-05    -14.648      0.000      -0.000      -0.000\nreview_scores_cleanliness     0.1346      0.002     83.462      0.000       0.131       0.138\nreview_scores_location       -0.0330      0.002    -19.147      0.000      -0.036      -0.030\nreview_scores_value          -0.0566      0.002    -27.877      0.000      -0.061      -0.053\ninstant_bookable              0.6482      0.003    220.862      0.000       0.642       0.654\n=============================================================================================\n\n\n\n\ninterpretation of Results\nThe Poisson regression model shows that the number of reviews is significantly associated with the number of days the unit has been listed, the number of bedrooms, the price, and the cleanliness score. The number of reviews is expected to increase by 0.0002 for each additional day the unit has been listed, by 0.2 for each additional bedroom, by 0.0001 for each additional dollar in price, and by 0.1 for each additional cleanliness score. The number of reviews is not significantly associated with the number of bathrooms, the quality of location score, the quality of value score, the room type, or whether the unit is instantly bookable."
  },
  {
    "objectID": "hw3_questions.html",
    "href": "hw3_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "hw3_questions.html#estimating-yogurt-preferences",
    "href": "hw3_questions.html#estimating-yogurt-preferences",
    "title": "Poisson Regression Examples",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\ntodo: import the data, maybe show the first few rows, and describe the data a bit.\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[ x_j' = [\\mathbbm{1}(\\text{Yogurt 1}), \\mathbbm{1}(\\text{Yogurt 2}), \\mathbbm{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\ntodo: reshape and prep the data\n\n\nEstimation\ntodo: Code up the log-likelihood function.\ntodo: Use optim() in R or optimize() in Python to find the MLEs for the 5 parameters (\\(\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p\\)). (Hint: you should find 2 positive and 1 negative product intercepts, a small positive coefficient estimate for featured, and a large negative coefficient estimate for price.)\n\n\nDiscussion\nWe learn…\ntodo: interpret the 3 product intercepts (which yogurt is most preferred?).\ntodo: use the estimated price coefficient as a dollar-per-util conversion factor. Use this conversion factor to calculate the dollar benefit between the most-preferred yogurt (the one with the highest intercept) and the least preferred yogurt (the one with the lowest intercept). This is a per-unit monetary measure of brand value.\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\ntodo: calculate the market shares in the market at the time the data were collected. Then, increase the price of yogurt 1 by $0.10 and use your fitted model to predict p(y|x) for each consumer and each product (this should be a matrix of \\(N \\times 4\\) estimated choice probabilities. Take the column averages to get the new, expected market shares that result from the $0.10 price increase to yogurt 1. Do the yogurt 1 market shares decrease?"
  },
  {
    "objectID": "hw3_questions.html#estimating-minivan-preferences",
    "href": "hw3_questions.html#estimating-minivan-preferences",
    "title": "Poisson Regression Examples",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\ntodo: download the dataset from here: http://goo.gl/5xQObB\ntodo: describe the data a bit. How many respondents took the conjoint survey? How many choice tasks did each respondent complete? How many alternatives were presented on each choice task? For each alternative.\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\n\nModel\ntodo: estimate a MNL model omitting the following levels to avoide multicollinearity (6 seats, 2ft cargo, and gas engine). Include price as a continuous variable. Show a table of coefficients and standard errors. You may use your own likelihood function from above, or you may use a function from a package/library to perform the estimation.\n\n\nResults\ntodo: Interpret the coefficients. Which features are more preferred?\ntodo: Use the price coefficient as a dollar-per-util conversion factor. What is the dollar value of 3ft of cargo space as compared to 2ft of cargo space?\ntodo: assume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\nhint: this example is taken from the “R 4 Marketing Research” book by Chapman and Feit. I believe the same example is present in the companion book titled “Python 4 Marketing Research”. I encourage you to attempt these questions on your own, but if you get stuck or would like to compare you results to “the answers,” you may consult the Chapman and Feit books."
  },
  {
    "objectID": "projects/Project 3/hw3_questions.html",
    "href": "projects/Project 3/hw3_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/Project 3/hw3_questions.html#estimating-yogurt-preferences",
    "href": "projects/Project 3/hw3_questions.html#estimating-yogurt-preferences",
    "title": "Poisson Regression Examples",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\ntodo: import the data, show the first few rows, and describe the data a bit.\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\n\nfile_path = \"/Users/peteratkins/Desktop/yogurt_data.csv\"\n\ndf = pd.read_csv(file_path)\n\ndf[['y1', 'y2', 'y3', 'f1', 'f2', 'f3', 'f4']] = df[['y1', 'y2', 'y3', 'f1', 'f2', 'f3', 'f4']].astype(int)\ndf[['p1', 'p2', 'p3', 'p4']] = df[['p1', 'p2', 'p3', 'p4']].astype(float)\n\nprint(df.head())\n\n   id  y1  y2  y3  y4  f1  f2  f3  f4     p1     p2     p3     p4\n0   1   0   0   0   1   0   0   0   0  0.108  0.081  0.061  0.079\n1   2   0   1   0   0   0   0   0   0  0.108  0.098  0.064  0.075\n2   3   0   1   0   0   0   0   0   0  0.108  0.098  0.061  0.086\n3   4   0   1   0   0   0   0   0   0  0.108  0.098  0.061  0.086\n4   5   0   1   0   0   0   0   0   0  0.125  0.098  0.049  0.079\n\n\n\n\n\n\n\n   id  y1  y2  y3  y4  f1  f2  f3  f4     p1     p2     p3     p4\n0   1   0   0   0   1   0   0   0   0  0.108  0.081  0.061  0.079\n1   2   0   1   0   0   0   0   0   0  0.108  0.098  0.064  0.075\n2   3   0   1   0   0   0   0   0   0  0.108  0.098  0.061  0.086\n3   4   0   1   0   0   0   0   0   0  0.108  0.098  0.061  0.086\n4   5   0   1   0   0   0   0   0   0  0.125  0.098  0.049  0.079\n\n\nThe vector of product features includes brand dummy variables for yogurts 1-3 with product product 4 omitted to avoid multi-collinearity. The ‘f’ dummy variable to indicates if a yogurt was featured, and the continuous variable ‘p’ indicates the yogurts’ prices per oz.\n\\[ x_j' = [\\mathbbm{1}(\\text{Yogurt 1}), \\mathbbm{1}(\\text{Yogurt 2}), \\mathbbm{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\n\nfile_path = \"/Users/peteratkins/Desktop/yogurt_data.csv\"\n\ndf = pd.read_csv(file_path)\n\ndf[['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4']] = df[['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4']].astype(int)\ndf[['p1', 'p2', 'p3', 'p4']] = df[['p1', 'p2', 'p3', 'p4']].astype(float)\n\n\ndf_long_y = pd.melt(df, id_vars=['id'], value_vars=['y1', 'y2', 'y3', 'y4'], var_name='yogurt', value_name='selected')\ndf_long_f = pd.melt(df, id_vars=['id'], value_vars=['f1', 'f2', 'f3', 'f4'], var_name='yogurt', value_name='Featured?')\ndf_long_p = pd.melt(df, id_vars=['id'], value_vars=['p1', 'p2', 'p3', 'p4'], var_name='yogurt', value_name='Price per Oz')\n\n\ndf_long_y['yogurt'] = df_long_y['yogurt'].str[1:]\ndf_long_f['yogurt'] = df_long_f['yogurt'].str[1:]\ndf_long_p['yogurt'] = df_long_p['yogurt'].str[1:]\n\n\ndf_long = pd.merge(df_long_y, df_long_f, on=['id', 'yogurt'])\ndf_long = pd.merge(df_long, df_long_p, on=['id', 'yogurt'])\n\n\ndf_long = df_long[df_long['selected'] == 1].drop(columns='selected')\n\n\ndf_long['yogurt'] = 'yogurt ' + df_long['yogurt']\n\ndf_long = df_long.sort_values(by='id')\n\nprint(df_long.head().to_string(index=False))\n\n id   yogurt  Featured?  Price per Oz\n  1 yogurt 4          0         0.079\n  2 yogurt 2          0         0.098\n  3 yogurt 2          0         0.098\n  4 yogurt 2          0         0.098\n  5 yogurt 2          0         0.098\n\n\n\n\n\n\n\n id   yogurt  Featured?  Price per Oz\n  1 yogurt 4          0         0.079\n  2 yogurt 2          0         0.098\n  3 yogurt 2          0         0.098\n  4 yogurt 2          0         0.098\n  5 yogurt 2          0         0.098\n\n\n\n\nEstimation\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\n\n\ndf_long['yogurt1'] = (df_long['yogurt'] == 'yogurt 1').astype(int)\ndf_long['yogurt2'] = (df_long['yogurt'] == 'yogurt 2').astype(int)\ndf_long['yogurt3'] = (df_long['yogurt'] == 'yogurt 3').astype(int)\n\nX = df_long[['yogurt1', 'yogurt2', 'yogurt3', 'Featured?', 'Price per Oz']].values\ny = (df_long['yogurt'] == 'yogurt 1').values\n\n\ndef log_likelihood(beta, X, y):\n    logit = expit(X @ beta)\n    return np.sum(y * np.log(logit + 1e-10) + (1 - y) * np.log(1 - logit + 1e-10))\n\n\nbeta_init = np.ones(X.shape[1]) * .1\n\n\nneg_log_likelihood = lambda beta: -log_likelihood(beta, X, y)\n\n\nres = minimize(neg_log_likelihood, beta_init, method='BFGS')\n\nbeta_hat = res.x\n\ntitles = ['beta_hat1 (yogurt1)', 'beta_hat2 (yogurt2)', 'beta_hat3 (yogurt3)', 'beta_hat_f (Featured?)', 'beta_hat_p (Price per Oz)']\n\nfor title, value in zip(titles, beta_hat):\n    print(f'{title} = {value}')\n\nbeta_hat1 (yogurt1) = 295.3739834941995\nbeta_hat2 (yogurt2) = -23.21840316512523\nbeta_hat3 (yogurt3) = -53.80907317457151\nbeta_hat_f (Featured?) = -29.610193648120337\nbeta_hat_p (Price per Oz) = -1525.4730223074298\n\n\n\n\n\n\n\nbeta_hat1 (yogurt1) = 289.10518542849934\nbeta_hat2 (yogurt2) = -27.096191386259015\nbeta_hat3 (yogurt3) = -7.097746907156205\nbeta_hat_f (Featured?) = -3.9822685151675525\nbeta_hat_p (Price per Oz) = -1554.0769216325868\n\n\n\n\nDiscussion\nThe yogurt with the highest likelihood of being selected is yogurt 1, as it has the largest coefficient.\n\n\nPer-unit monetary measure of brand value\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\n\nbeta_hat_p = beta_hat[titles.index('beta_hat_p (Price per Oz)')]\n\nbeta_hat1 = beta_hat[titles.index('beta_hat1 (yogurt1)')]\nbeta_hat2 = beta_hat[titles.index('beta_hat2 (yogurt2)')]\nbeta_hat3 = beta_hat[titles.index('beta_hat3 (yogurt3)')]\n\ndollar_per_utiity = 1 / beta_hat_p\n\nutility_difference = beta_hat1 - min(beta_hat1, beta_hat2, beta_hat3)\n\ndollar_benegit = utility_difference * dollar_per_utiity\n\nprint(f'The dollar benefit between the most-preferred yogurt and the least preferred yogurt is ${dollar_benegit:.2f}')\n\nThe dollar benefit between the most-preferred yogurt and the least preferred yogurt is $-0.20\n\n\n\n\n\n\n\nThe dollar benefit between the most-preferred yogurt and the least preferred yogurt is $-0.20\n\n\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import softmax\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\nmarket_shares_current = df_long['yogurt'].value_counts(normalize=True)\n\naverage_price = df_long.groupby('yogurt')['Price per Oz'].mean()\n\ndf_summary = pd.DataFrame({\n    'Market Share': market_shares_current,\n    'Price': average_price})\n\ndf_summary['Market Share'] = df_summary['Market Share'].apply(lambda x: f'{x*100:.2f}%')\n\ndf_summary['Price'] = df_summary['Price'].apply(lambda x: f'${x:.2f}')\n\nprint(\"Existing Price and Market Share\")\nprint(df_summary)\n\nfrom sklearn.linear_model import LogisticRegression\n\ndf_long['yogurt_num'] = df_long['yogurt'].map({'yogurt 1': 0, 'yogurt 2': 1, 'yogurt 3': 2, 'yogurt 4': 3})\n\nX = df_long[['Featured?', 'Price per Oz']].values\ny = df_long['yogurt_num'].values\n\nmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs')\nmodel.fit(X, y)\n\ndf_long_new = df_long.copy()\ndf_long_new.loc[df_long_new['yogurt'] == 'yogurt 1', 'Price per Oz'] += 0.10\n\nX_new = df_long_new[['Featured?', 'Price per Oz']].values\nprobabilities_new = model.predict_proba(X_new)\n\nmarket_shares_new = probabilities_new.mean(axis=0)\n\naverage_price_new = df_long_new.groupby('yogurt')['Price per Oz'].mean()\n\ndf_summary_new = pd.DataFrame({\n    'Market Share': market_shares_new,\n    'Price': average_price_new\n}, index=['yogurt 1', 'yogurt 2', 'yogurt 3', 'yogurt 4'])\n\ndf_summary_new['Market Share'] = df_summary_new['Market Share'].apply(lambda x: f'{x*100:.2f}%')\n\ndf_summary_new['Price'] = df_summary_new['Price'].apply(lambda x: f'${x:.2f}')\n\nprint(\" \")\nprint(\"New Price and Market Share\")\nprint(df_summary_new)\n\nExisting Price and Market Share\n         Market Share  Price\nyogurt                      \nyogurt 1       34.20%  $0.10\nyogurt 2       40.12%  $0.08\nyogurt 3        2.92%  $0.04\nyogurt 4       22.76%  $0.08\n \nNew Price and Market Share\n         Market Share  Price\nyogurt 1       43.50%  $0.20\nyogurt 2       34.14%  $0.08\nyogurt 3        2.51%  $0.04\nyogurt 4       19.84%  $0.08\n\n\n\n\n\n\n\nExisting Price and Market Share\n         Market Share  Price\nyogurt                      \nyogurt 1       34.20%  $0.10\nyogurt 2       40.12%  $0.08\nyogurt 3        2.92%  $0.04\nyogurt 4       22.76%  $0.08\n \nNew Price and Market Share\n         Market Share  Price\nyogurt 1       43.50%  $0.20\nyogurt 2       34.14%  $0.08\nyogurt 3        2.51%  $0.04\nyogurt 4       19.84%  $0.08\n \nThe Market Share of Yogurt 1 has increased despite the price increase"
  },
  {
    "objectID": "projects/Project 3/hw3_questions.html#estimating-minivan-preferences",
    "href": "projects/Project 3/hw3_questions.html#estimating-minivan-preferences",
    "title": "Poisson Regression Examples",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\n\n\n\nCode\n\n\n### _todo: download the dataset from here:_ http://goo.gl/5xQObB \n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\n\nconjoint_data = \"/Users/peteratkins/Desktop/chapter13conjoint.csv\"\n\ndf_conjoint = pd.read_csv(conjoint_data)\n\nprint(df_conjoint.head())\n\nnum_respondents = df_conjoint['resp.id'].nunique()\nnum_tasks = df_conjoint['ques'].nunique()\nalternatives_per_task = df_conjoint.groupby(['resp.id', 'ques'])['alt'].nunique().reset_index()\nalternatives_per_task.columns = ['Respondent ID', 'Question', 'Number of Alternatives']\n\nprint(\"\")\nprint(f\"Number of Respondents: {num_respondents}\")\nprint(f\"Number of Tasks per Respondent: {num_tasks}\")\nprint(f\"Number of Alternatives per Task: {alternatives_per_task['Number of Alternatives'].unique()}\")\n\n   resp.id  ques  alt carpool  seat cargo  eng  price  choice\n0        1     1    1     yes     6   2ft  gas     35       0\n1        1     1    2     yes     8   3ft  hyb     30       0\n2        1     1    3     yes     6   3ft  gas     30       1\n3        1     2    1     yes     6   2ft  gas     30       0\n4        1     2    2     yes     7   3ft  gas     35       1\n\nNumber of Respondents: 200\nNumber of Tasks per Respondent: 15\nNumber of Alternatives per Task: [3]\n\n\n\n\n\n\n\n   resp.id  ques  alt carpool  seat cargo  eng  price  choice\n0        1     1    1     yes     6   2ft  gas     35       0\n1        1     1    2     yes     8   3ft  hyb     30       0\n2        1     1    3     yes     6   3ft  gas     30       1\n3        1     2    1     yes     6   2ft  gas     30       0\n4        1     2    2     yes     7   3ft  gas     35       1\n\nNumber of Respondents: 200\nNumber of Tasks per Respondent: 15\nNumber of Alternatives per Task: [3]\n The attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\n\n\n\nModel\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\nfrom statsmodels.discrete.discrete_model import Logit\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import LabelEncoder\n\nlabelencoder = LabelEncoder()\nfor col in df_conjoint.columns:\n    if df_conjoint[col].dtype == object:\n        df_conjoint[col] = labelencoder.fit_transform(df_conjoint[col])\n\nX = df_conjoint.drop('choice', axis=1)\ny = df_conjoint['choice']\n\nmodel = Logit(y, X)\nresult = model.fit()\n\nprint(result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.603397\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 choice   No. Observations:                 9000\nModel:                          Logit   Df Residuals:                     8992\nMethod:                           MLE   Df Model:                            7\nDate:                Sat, 11 May 2024   Pseudo R-squ.:                 0.05203\nTime:                        16:39:31   Log-Likelihood:                -5430.6\nconverged:                       True   LL-Null:                       -5728.6\nCovariance Type:            nonrobust   LLR p-value:                1.673e-124\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nresp.id        0.0009      0.000      2.191      0.028    9.25e-05       0.002\nques           0.0122      0.005      2.299      0.021       0.002       0.023\nalt            0.1241      0.028      4.457      0.000       0.070       0.179\ncarpool        0.0327      0.051      0.642      0.521      -0.067       0.133\nseat           0.1805      0.021      8.632      0.000       0.140       0.221\ncargo          0.4571      0.046      9.872      0.000       0.366       0.548\neng            0.3216      0.028     11.299      0.000       0.266       0.377\nprice         -0.0864      0.004    -19.709      0.000      -0.095      -0.078\n==============================================================================\n\n\n\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.603397\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 choice   No. Observations:                 9000\nModel:                          Logit   Df Residuals:                     8992\nMethod:                           MLE   Df Model:                            7\nDate:                Sat, 11 May 2024   Pseudo R-squ.:                 0.05203\nTime:                        16:39:31   Log-Likelihood:                -5430.6\nconverged:                       True   LL-Null:                       -5728.6\nCovariance Type:            nonrobust   LLR p-value:                1.673e-124\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nresp.id        0.0009      0.000      2.191      0.028    9.25e-05       0.002\nques           0.0122      0.005      2.299      0.021       0.002       0.023\nalt            0.1241      0.028      4.457      0.000       0.070       0.179\ncarpool        0.0327      0.051      0.642      0.521      -0.067       0.133\nseat           0.1805      0.021      8.632      0.000       0.140       0.221\ncargo          0.4571      0.046      9.872      0.000       0.366       0.548\neng            0.3216      0.028     11.299      0.000       0.266       0.377\nprice         -0.0864      0.004    -19.709      0.000      -0.095      -0.078\n==============================================================================\n\nThe results show that the number of seats, cargo space, and engine type are all significant predictors of the minivan choice. The price coefficient is negative, indicating that higher prices reduce the probability of choosing a minivan. Ability to carry 3ft of cargo space is the most preferred feature, followed by a hybrid engine, and 7 seats.\n\n\n\n\nDollar Value of 3ft Cargo Space\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\n\ndollar_value = (coef_cargo_3ft / -coef_price) *(3-2)\nprint(f\"The dollar value of 3ft of cargo space as compared to 2ft of cargo space is ${dollar_value:.2f}\")\n\nThe dollar value of 3ft of cargo space as compared to 2ft of cargo space is $5.29\n\n\n\n\n\n\n\nThe dollar value of 3ft of cargo space as compared to 2ft of cargo space is $5.29\n\n\n\n\nMarket Shares\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.api import Logit\n\nmarket = pd.DataFrame({\n    'Minivan': ['A', 'B', 'C', 'D', 'E', 'F'],\n    'seat_7': [1, 0, 0, 1, 0, 1],\n    'seat_8': [0, 0, 1, 0, 0, 0],\n    'cargo_3ft': [0, 0, 0, 1, 0, 0],\n    'eng_gas': [0, 1, 1, 1, 0, 0],\n    'eng_hyb': [1, 0, 0, 0, 0, 1],\n    'eng_elec': [0, 0, 0, 0, 1, 0],\n    'price': [30, 30, 30, 40, 40, 35]\n})\n\ncoefficients = pd.Series([0.1805, 0.1805, 0.4571, 0.3216, 0.3216, 0.3216, -0.0864], index=['seat_7', 'seat_8', 'cargo_3ft', 'eng_gas', 'eng_hyb', 'eng_elec', 'price'])\n\nmarket['Utility'] = np.dot(market[['seat_7', 'seat_8', 'cargo_3ft', 'eng_gas', 'eng_hyb', 'eng_elec', 'price']], coefficients)\n\nmarket['Probability'] = np.exp(market['Utility']) / np.sum(np.exp(market['Utility']))\n\nmarket['Market Share'] = market['Probability']\n\nprint(market.assign(Market_Share_Percent = [f'{x * 100:.2f}%' for x in market['Market Share']])[['Minivan', 'Market_Share_Percent']].to_string(index=False))\n\nMinivan Market_Share_Percent\n      A               22.21%\n      B               18.55%\n      C               22.21%\n      D               14.79%\n      E                7.82%\n      F               14.42%\n\n\n\n\n\n\n\nMinivan Market_Share_Percent\n      A               22.21%\n      B               18.55%\n      C               22.21%\n      D               14.79%\n      E                7.82%\n      F               14.42%"
  },
  {
    "objectID": "projects/Project4/hw4_questions.html",
    "href": "projects/Project4/hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "Data\nThe data imported reflects 10 different brands - labeled 1-10, and 940 different products (‘id’ column’) accross those 10 brands. The data includes the following columns: Satisfaction (on a scale of 1 being the lowest and 5 being the highest), and nine binary variables (1 is True, 0 is False) that represent different aspects of customer satisfaction pertaining to certian aspects of a payment card on customer satisfaction with that payment card. These 9 variables are: satisfaction, trust, build, differs, easy, appealing, rewarding, popular, service, and impact. This report will create a Pearson correlation, Polychoric correlations, standardized multiple regression coefficients, LMG/Shapley Values, Johnson’s epilion, and display the mean decrease in the gini coefficient from a random forest. Lastly, I will make a matrix with all the measures in the rows and the correlation type in the columns.\n\n\nCode\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\nfrom scipy.optimize import minimize_scalar\nimport pingouin as pg\nimport shap\nimport xgboost as xgb\nfrom itertools import combinations\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\ndrivers_data = \"/Users/peteratkins/Desktop/drivers_data.csv\"\ndf_drivers = pd.read_csv(drivers_data)\n\nprint(df_drivers.head())\n\n   brand   id  satisfaction  trust  build  differs  easy  appealing  \\\n0      1   98             3      1      0        1     1          1   \n1      1  179             5      0      0        0     0          0   \n2      1  197             3      1      0        0     1          1   \n3      1  317             1      0      0        0     0          1   \n4      1  356             4      1      1        1     1          1   \n\n   rewarding  popular  service  impact  \n0          0        0        1       0  \n1          0        0        0       0  \n2          1        0        1       1  \n3          0        1        1       1  \n4          1        1        1       1  \n\n\n\n\nPearson Correlation\n\n\nCode\n\n\npearson_corr = df_drivers.corr()\npearson_satisfaction_corr = pearson_corr['satisfaction']\npearson_satisfaction_corr = pearson_satisfaction_corr.drop(['satisfaction', 'brand', 'id'])\npearson_satisfaction_corr = pearson_satisfaction_corr.to_frame().reset_index()\npearson_satisfaction_corr.columns = ['Perception', 'Pearson Correlation']\npearson_satisfaction_corr['Pearson Correlation'] = pearson_satisfaction_corr['Pearson Correlation'].map('{:.1%}'.format)\n\n\n\n\n\n\nPerception Pearson Correlation\n     trust               25.6%\n     build               19.2%\n   differs               18.5%\n      easy               21.3%\n appealing               20.8%\n rewarding               19.5%\n   popular               17.1%\n   service               25.1%\n    impact               25.5%\n\n\n\n\n\n\n\n\n\n\n\nPolychoric Correlations\n\n\nCode\n\n\nspearman_corr = df_drivers.corr(method='spearman')\nspearman_satisfaction_corr = spearman_corr['satisfaction']\nspearman_satisfaction_corr = spearman_satisfaction_corr.drop(['satisfaction', 'brand', 'id'])\nspearman_satisfaction_corr = spearman_satisfaction_corr.to_frame().reset_index()\nspearman_satisfaction_corr.columns = ['Perception', 'Polychoric Correlation']\nspearman_satisfaction_corr['Polychoric Correlation'] = spearman_satisfaction_corr['Polychoric Correlation'].map('{:.1%}'.format)\nprint(spearman_satisfaction_corr.to_string(index=False))\n\nPerception Polychoric Correlation\n     trust                  25.3%\n     build                  19.5%\n   differs                  19.0%\n      easy                  21.2%\n appealing                  20.4%\n rewarding                  19.9%\n   popular                  17.1%\n   service                  25.2%\n    impact                  26.1%\n\n\n\n\n\n\n\nPerception Polychoric Correlation\n     trust                  25.3%\n     build                  19.5%\n   differs                  19.0%\n      easy                  21.2%\n appealing                  20.4%\n rewarding                  19.9%\n   popular                  17.1%\n   service                  25.2%\n    impact                  26.1%\n\n\n\n\n\n\n\n\n\n\n\nStandardized Multiple Regression Coefficients\n\n\nCode\n\n\nformula = 'satisfaction ~ trust + build + differs + easy + appealing + rewarding + popular + service + impact'\n\nmodel = ols(formula, data=df_drivers).fit()\n\nstandardized_coeffs = model.params / df_drivers.std()\n\npredictor_coeffs = standardized_coeffs.drop(['Intercept', 'satisfaction', 'brand', 'id'])\n\npredictor_coeffs = predictor_coeffs.to_frame().reset_index()\n\npredictor_coeffs.columns = ['Perception', 'Standardized Coefficient']\n\norder = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\npredictor_coeffs = predictor_coeffs.set_index('Perception').reindex(order).reset_index()\n\npredictor_coeffs['Standardized Coefficient'] = predictor_coeffs['Standardized Coefficient'].map('{:.1%}'.format)\n\nprint(predictor_coeffs.to_string(index=False))\n\nPerception Standardized Coefficient\n     trust                    54.8%\n     build                     9.4%\n   differs                    14.7%\n      easy                    10.3%\n appealing                    16.0%\n rewarding                     2.4%\n   popular                     7.8%\n   service                    41.6%\n    impact                    67.9%\n\n\n\n\n\n\n\nPerception Standardized Coefficient\n rewarding                     2.4%\n   popular                     7.8%\n     build                     9.4%\n      easy                    10.3%\n   differs                    14.7%\n appealing                    16.0%\n   service                    41.6%\n     trust                    54.8%\n    impact                    67.9%\n\n\n\n\n\n\n\n\n\n\n\nLMG/Shapley Values\n\n\nCode\n\n\nX = df_drivers[['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']]\ny = df_drivers['satisfaction']\n\nmodel = xgb.XGBRegressor().fit(X, y)\n\nexplainer = shap.Explainer(model)\nshap_values = explainer(X)\n\nmean_shap_values = np.abs(shap_values.values).mean(axis=0)\n\nshap_df = pd.DataFrame(mean_shap_values, index=X.columns, columns=['LMG Value'])\n\nshap_df = shap_df.reset_index()\n\nshap_df.columns = ['Perception', 'LMG Value']\n\norder = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\nshap_df = shap_df.set_index('Perception').reindex(order).reset_index()\n\nshap_df['LMG Value'] = shap_df['LMG Value'].map('{:.1%}'.format)\n\nprint(shap_df.to_string(index=False))\n\nPerception LMG Value\n     trust     17.5%\n     build      5.7%\n   differs      5.3%\n      easy      5.0%\n appealing      6.8%\n rewarding      5.1%\n   popular      6.2%\n   service     10.6%\n    impact     18.1%\n\n\n\n\n\n\n\nPerception LMG Value\n      easy      5.0%\n rewarding      5.1%\n   differs      5.3%\n     build      5.7%\n   popular      6.2%\n appealing      6.8%\n   service     10.6%\n     trust     17.5%\n    impact     18.1%\n\n\n\n\n\n\n\n\n\n\n\nJohnson’s Epsilon\n\n\nCode\n\n\ndef johnsons_epsilon(X, y):\n    n_features = X.shape[1]\n    indices = np.arange(n_features)\n    subsets = [combinations(indices, i) for i in range(1, n_features + 1)]\n    lr = LinearRegression()\n\n    epsilon = np.zeros(n_features)\n    for subset in subsets:\n        for combination in subset:\n            lr.fit(X[:, combination], y)\n            r_squared = lr.score(X[:, combination], y)\n            for index in combination:\n                epsilon[index] += r_squared\n\n    epsilon /= epsilon.sum()  # Normalize so that epsilon values sum up to 1\n    return epsilon\n\nX = df_drivers[['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']].values\ny = df_drivers['satisfaction'].values\n\nepsilon = johnsons_epsilon(X, y)\n\nepsilon_df = pd.DataFrame(epsilon, index=['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact'], columns=['Epsilon Value'])\n\nepsilon_df = epsilon_df.reset_index()\n\nepsilon_df.columns = ['Perception', 'Epsilon Value']\n\norder = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\nepsilon_df = epsilon_df.set_index('Perception').reindex(order).reset_index()\n\nepsilon_df['Epsilon Value'] = epsilon_df['Epsilon Value'].map('{:.1%}'.format)\n\nprint(epsilon_df.to_string(index=False))\n\nPerception Epsilon Value\n     trust         11.6%\n     build         10.9%\n   differs         10.9%\n      easy         10.9%\n appealing         10.9%\n rewarding         10.8%\n   popular         10.8%\n   service         11.4%\n    impact         11.8%\n\n\n\n\n\n\n\nPerception Epsilon Value\n   popular         10.8%\n rewarding         10.8%\n     build         10.9%\n   differs         10.9%\n      easy         10.9%\n appealing         10.9%\n   service         11.4%\n     trust         11.6%\n    impact         11.8%\n\n\n\n\n\n\n\n\n\n\n\nMean Decrease in the Gini Coefficient\n\n\nCode\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nX = df_drivers[['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']]\ny = df_drivers['satisfaction']\n\nmodel = RandomForestRegressor().fit(X, y)\n\nimportances = model.feature_importances_\n\nimportances_df = pd.DataFrame(importances, index=X.columns, columns=['Gini Decrease'])\n\nimportances_df = importances_df.reset_index()\n\nimportances_df.columns = ['Perception', 'Gini Decrease']\n\norder = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\nimportances_df = importances_df.set_index('Perception').reindex(order).reset_index()\n\nimportances_df['Gini Decrease'] = importances_df['Gini Decrease'].map('{:.1%}'.format)\n\nprint(importances_df.to_string(index=False))\n\nPerception Gini Decrease\n     trust         15.5%\n     build          9.8%\n   differs          9.0%\n      easy         10.2%\n appealing          8.3%\n rewarding         10.1%\n   popular          9.7%\n   service         13.5%\n    impact         14.0%\n\n\n\n\n\n\n\nPerception Gini Decrease\n     trust         15.4%\n     build          9.9%\n   differs          9.4%\n      easy         10.3%\n appealing          8.4%\n rewarding         10.2%\n   popular          9.1%\n   service         13.7%\n    impact         13.7%\n\n\n\n\n\n\n\n\n\n\n\nMatrix with all the measures\nTrust, service, and impact have the highest pearson correlation with satisfaction. Trust, service, and impact also have the highest polychoric correlation with satisfaction. Trust, service, and impact have the highest standardized coefficients. Trust, service, and impact have the highest LMG values. Trust, service, and impact have the highest Johnson’s epsilon values. Trust, service, and impact have the highest mean decrease in the gini coefficient from a random forest. This indicates that those are the most important predictors of satisfaction.\n\n\nCode\n\n\nmerged_df = pd.merge(pearson_satisfaction_corr, spearman_satisfaction_corr, on='Perception', how='outer', suffixes=('_pearson', '_polychoric'))\nmerged_df = pd.merge(merged_df, predictor_coeffs, on='Perception', how='outer')\nmerged_df = pd.merge(merged_df, shap_df, on='Perception', how='outer')\nmerged_df = pd.merge(merged_df, epsilon_df, on='Perception', how='outer')\nmerged_df = pd.merge(merged_df, importances_df, on='Perception', how='outer')\n\nnumerical_columns = merged_df.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numerical_columns:\n    merged_df[col] = merged_df[col].apply('{:.1%}'.format)\n\nstyled_df = merged_df.style.set_table_styles([\n    {'selector': 'th', 'props': [('background', '#606060'), ('color', 'white'), ('font-family', 'verdana')]},\n    {'selector': 'td', 'props': [('font-family', 'verdana')]}\n])\n\nstyled_df\n\n\n\n\n\n\n\n \nPerception\nPearson Correlation\nPolychoric Correlation\nStandardized Coefficient\nLMG Value\nEpsilon Value\nGini Decrease\n\n\n\n\n0\ntrust\n25.6%\n25.3%\n54.8%\n17.5%\n11.6%\n15.4%\n\n\n1\nbuild\n19.2%\n19.5%\n9.4%\n5.7%\n10.9%\n9.9%\n\n\n2\ndiffers\n18.5%\n19.0%\n14.7%\n5.3%\n10.9%\n9.4%\n\n\n3\neasy\n21.3%\n21.2%\n10.3%\n5.0%\n10.9%\n10.3%\n\n\n4\nappealing\n20.8%\n20.4%\n16.0%\n6.8%\n10.9%\n8.4%\n\n\n5\nrewarding\n19.5%\n19.9%\n2.4%\n5.1%\n10.8%\n10.2%\n\n\n6\npopular\n17.1%\n17.1%\n7.8%\n6.2%\n10.8%\n9.1%\n\n\n7\nservice\n25.1%\n25.2%\n41.6%\n10.6%\n11.4%\n13.7%\n\n\n8\nimpact\n25.5%\n26.1%\n67.9%\n18.1%\n11.8%\n13.7%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nPerception\nPearson Correlation\nPolychoric Correlation\nStandardized Coefficient\nLMG Value\nEpsilon Value\nGini Decrease\n\n\n\n\n0\ntrust\n25.6%\n25.3%\n54.8%\n17.5%\n11.6%\n15.4%\n\n\n1\nbuild\n19.2%\n19.5%\n9.4%\n5.7%\n10.9%\n9.9%\n\n\n2\ndiffers\n18.5%\n19.0%\n14.7%\n5.3%\n10.9%\n9.4%\n\n\n3\neasy\n21.3%\n21.2%\n10.3%\n5.0%\n10.9%\n10.3%\n\n\n4\nappealing\n20.8%\n20.4%\n16.0%\n6.8%\n10.9%\n8.4%\n\n\n5\nrewarding\n19.5%\n19.9%\n2.4%\n5.1%\n10.8%\n10.2%\n\n\n6\npopular\n17.1%\n17.1%\n7.8%\n6.2%\n10.8%\n9.1%\n\n\n7\nservice\n25.1%\n25.2%\n41.6%\n10.6%\n11.4%\n13.7%\n\n\n8\nimpact\n25.5%\n26.1%\n67.9%\n18.1%\n11.8%\n13.7%"
  },
  {
    "objectID": "projects/Project3/hw3_questions.html",
    "href": "projects/Project3/hw3_questions.html",
    "title": "MNL & Conjoint Analysis",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/Project3/hw3_questions.html#estimating-yogurt-preferences",
    "href": "projects/Project3/hw3_questions.html#estimating-yogurt-preferences",
    "title": "MNL & Conjoint Analysis",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\ntodo: import the data, show the first few rows, and describe the data a bit.\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\n\nfile_path = \"/Users/peteratkins/Desktop/yogurt_data.csv\"\n\ndf = pd.read_csv(file_path)\n\ndf[['y1', 'y2', 'y3', 'f1', 'f2', 'f3', 'f4']] = df[['y1', 'y2', 'y3', 'f1', 'f2', 'f3', 'f4']].astype(int)\ndf[['p1', 'p2', 'p3', 'p4']] = df[['p1', 'p2', 'p3', 'p4']].astype(float)\n\nprint(df.head())\n\n   id  y1  y2  y3  y4  f1  f2  f3  f4     p1     p2     p3     p4\n0   1   0   0   0   1   0   0   0   0  0.108  0.081  0.061  0.079\n1   2   0   1   0   0   0   0   0   0  0.108  0.098  0.064  0.075\n2   3   0   1   0   0   0   0   0   0  0.108  0.098  0.061  0.086\n3   4   0   1   0   0   0   0   0   0  0.108  0.098  0.061  0.086\n4   5   0   1   0   0   0   0   0   0  0.125  0.098  0.049  0.079\n\n\n\n\n\n\n\n   id  y1  y2  y3  y4  f1  f2  f3  f4     p1     p2     p3     p4\n0   1   0   0   0   1   0   0   0   0  0.108  0.081  0.061  0.079\n1   2   0   1   0   0   0   0   0   0  0.108  0.098  0.064  0.075\n2   3   0   1   0   0   0   0   0   0  0.108  0.098  0.061  0.086\n3   4   0   1   0   0   0   0   0   0  0.108  0.098  0.061  0.086\n4   5   0   1   0   0   0   0   0   0  0.125  0.098  0.049  0.079\n\n\nThe vector of product features includes brand dummy variables for yogurts 1-3 with product product 4 omitted to avoid multi-collinearity. The ‘f’ dummy variable to indicates if a yogurt was featured, and the continuous variable ‘p’ indicates the yogurts’ prices per oz.\n\\[ x_j' = [\\mathbbm{1}(\\text{Yogurt 1}), \\mathbbm{1}(\\text{Yogurt 2}), \\mathbbm{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\n\nfile_path = \"/Users/peteratkins/Desktop/yogurt_data.csv\"\n\ndf = pd.read_csv(file_path)\n\ndf[['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4']] = df[['y1', 'y2', 'y3', 'y4', 'f1', 'f2', 'f3', 'f4']].astype(int)\ndf[['p1', 'p2', 'p3', 'p4']] = df[['p1', 'p2', 'p3', 'p4']].astype(float)\n\n\ndf_long_y = pd.melt(df, id_vars=['id'], value_vars=['y1', 'y2', 'y3', 'y4'], var_name='yogurt', value_name='selected')\ndf_long_f = pd.melt(df, id_vars=['id'], value_vars=['f1', 'f2', 'f3', 'f4'], var_name='yogurt', value_name='Featured?')\ndf_long_p = pd.melt(df, id_vars=['id'], value_vars=['p1', 'p2', 'p3', 'p4'], var_name='yogurt', value_name='Price per Oz')\n\n\ndf_long_y['yogurt'] = df_long_y['yogurt'].str[1:]\ndf_long_f['yogurt'] = df_long_f['yogurt'].str[1:]\ndf_long_p['yogurt'] = df_long_p['yogurt'].str[1:]\n\n\ndf_long = pd.merge(df_long_y, df_long_f, on=['id', 'yogurt'])\ndf_long = pd.merge(df_long, df_long_p, on=['id', 'yogurt'])\n\n\ndf_long = df_long[df_long['selected'] == 1].drop(columns='selected')\n\n\ndf_long['yogurt'] = 'yogurt ' + df_long['yogurt']\n\ndf_long = df_long.sort_values(by='id')\n\nprint(df_long.head().to_string(index=False))\n\n id   yogurt  Featured?  Price per Oz\n  1 yogurt 4          0         0.079\n  2 yogurt 2          0         0.098\n  3 yogurt 2          0         0.098\n  4 yogurt 2          0         0.098\n  5 yogurt 2          0         0.098\n\n\n\n\n\n\n\n id   yogurt  Featured?  Price per Oz\n  1 yogurt 4          0         0.079\n  2 yogurt 2          0         0.098\n  3 yogurt 2          0         0.098\n  4 yogurt 2          0         0.098\n  5 yogurt 2          0         0.098\n\n\n\n\nEstimation\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\n\n\ndf_long['yogurt1'] = (df_long['yogurt'] == 'yogurt 1').astype(int)\ndf_long['yogurt2'] = (df_long['yogurt'] == 'yogurt 2').astype(int)\ndf_long['yogurt3'] = (df_long['yogurt'] == 'yogurt 3').astype(int)\n\nX = df_long[['yogurt1', 'yogurt2', 'yogurt3', 'Featured?', 'Price per Oz']].values\ny = (df_long['yogurt'] == 'yogurt 1').values\n\n\ndef log_likelihood(beta, X, y):\n    logit = expit(X @ beta)\n    return np.sum(y * np.log(logit + 1e-10) + (1 - y) * np.log(1 - logit + 1e-10))\n\n\nbeta_init = np.ones(X.shape[1]) * .1\n\n\nneg_log_likelihood = lambda beta: -log_likelihood(beta, X, y)\n\n\nres = minimize(neg_log_likelihood, beta_init, method='BFGS')\n\nbeta_hat = res.x\n\ntitles = ['beta_hat1 (yogurt1)', 'beta_hat2 (yogurt2)', 'beta_hat3 (yogurt3)', 'beta_hat_f (Featured?)', 'beta_hat_p (Price per Oz)']\n\nfor title, value in zip(titles, beta_hat):\n    print(f'{title} = {value}')\n\nbeta_hat1 (yogurt1) = 295.3739834941995\nbeta_hat2 (yogurt2) = -23.21840316512523\nbeta_hat3 (yogurt3) = -53.80907317457151\nbeta_hat_f (Featured?) = -29.610193648120337\nbeta_hat_p (Price per Oz) = -1525.4730223074298\n\n\n\n\n\n\n\nbeta_hat1 (yogurt1) = 289.10518542849934\nbeta_hat2 (yogurt2) = -27.096191386259015\nbeta_hat3 (yogurt3) = -7.097746907156205\nbeta_hat_f (Featured?) = -3.9822685151675525\nbeta_hat_p (Price per Oz) = -1554.0769216325868\n\n\n\n\nDiscussion\nThe yogurt with the highest likelihood of being selected is yogurt 1, as it has the largest coefficient.\n\n\nPer-unit monetary measure of brand value\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\n\nbeta_hat_p = beta_hat[titles.index('beta_hat_p (Price per Oz)')]\n\nbeta_hat1 = beta_hat[titles.index('beta_hat1 (yogurt1)')]\nbeta_hat2 = beta_hat[titles.index('beta_hat2 (yogurt2)')]\nbeta_hat3 = beta_hat[titles.index('beta_hat3 (yogurt3)')]\n\ndollar_per_utiity = 1 / beta_hat_p\n\nutility_difference = beta_hat1 - min(beta_hat1, beta_hat2, beta_hat3)\n\ndollar_benegit = utility_difference * dollar_per_utiity\n\nprint(f'The dollar benefit between the most-preferred yogurt and the least preferred yogurt is ${dollar_benegit:.2f}')\n\nThe dollar benefit between the most-preferred yogurt and the least preferred yogurt is $-0.20\n\n\n\n\n\n\n\nThe dollar benefit between the most-preferred yogurt and the least preferred yogurt is $-0.20\n\n\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import softmax\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\nmarket_shares_current = df_long['yogurt'].value_counts(normalize=True)\n\naverage_price = df_long.groupby('yogurt')['Price per Oz'].mean()\n\ndf_summary = pd.DataFrame({\n    'Market Share': market_shares_current,\n    'Price': average_price})\n\ndf_summary['Market Share'] = df_summary['Market Share'].apply(lambda x: f'{x*100:.2f}%')\n\ndf_summary['Price'] = df_summary['Price'].apply(lambda x: f'${x:.2f}')\n\nprint(\"Existing Price and Market Share\")\nprint(df_summary)\n\nfrom sklearn.linear_model import LogisticRegression\n\ndf_long['yogurt_num'] = df_long['yogurt'].map({'yogurt 1': 0, 'yogurt 2': 1, 'yogurt 3': 2, 'yogurt 4': 3})\n\nX = df_long[['Featured?', 'Price per Oz']].values\ny = df_long['yogurt_num'].values\n\nmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs')\nmodel.fit(X, y)\n\ndf_long_new = df_long.copy()\ndf_long_new.loc[df_long_new['yogurt'] == 'yogurt 1', 'Price per Oz'] += 0.10\n\nX_new = df_long_new[['Featured?', 'Price per Oz']].values\nprobabilities_new = model.predict_proba(X_new)\n\nmarket_shares_new = probabilities_new.mean(axis=0)\n\naverage_price_new = df_long_new.groupby('yogurt')['Price per Oz'].mean()\n\ndf_summary_new = pd.DataFrame({\n    'Market Share': market_shares_new,\n    'Price': average_price_new\n}, index=['yogurt 1', 'yogurt 2', 'yogurt 3', 'yogurt 4'])\n\ndf_summary_new['Market Share'] = df_summary_new['Market Share'].apply(lambda x: f'{x*100:.2f}%')\n\ndf_summary_new['Price'] = df_summary_new['Price'].apply(lambda x: f'${x:.2f}')\n\nprint(\" \")\nprint(\"New Price and Market Share\")\nprint(df_summary_new)\n\nExisting Price and Market Share\n         Market Share  Price\nyogurt                      \nyogurt 1       34.20%  $0.10\nyogurt 2       40.12%  $0.08\nyogurt 3        2.92%  $0.04\nyogurt 4       22.76%  $0.08\n \nNew Price and Market Share\n         Market Share  Price\nyogurt 1       43.50%  $0.20\nyogurt 2       34.14%  $0.08\nyogurt 3        2.51%  $0.04\nyogurt 4       19.84%  $0.08\n\n\n\n\n\n\n\nExisting Price and Market Share\n         Market Share  Price\nyogurt                      \nyogurt 1       34.20%  $0.10\nyogurt 2       40.12%  $0.08\nyogurt 3        2.92%  $0.04\nyogurt 4       22.76%  $0.08\n \nNew Price and Market Share\n         Market Share  Price\nyogurt 1       43.50%  $0.20\nyogurt 2       34.14%  $0.08\nyogurt 3        2.51%  $0.04\nyogurt 4       19.84%  $0.08\n \nThe Market Share of Yogurt 1 has increased despite the price increase"
  },
  {
    "objectID": "projects/Project3/hw3_questions.html#estimating-minivan-preferences",
    "href": "projects/Project3/hw3_questions.html#estimating-minivan-preferences",
    "title": "MNL & Conjoint Analysis",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\n\n\n\nCode\n\n\n### _todo: download the dataset from here:_ http://goo.gl/5xQObB \n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\n\nconjoint_data = \"/Users/peteratkins/Desktop/chapter13conjoint.csv\"\n\ndf_conjoint = pd.read_csv(conjoint_data)\n\nprint(df_conjoint.head())\n\nnum_respondents = df_conjoint['resp.id'].nunique()\nnum_tasks = df_conjoint['ques'].nunique()\nalternatives_per_task = df_conjoint.groupby(['resp.id', 'ques'])['alt'].nunique().reset_index()\nalternatives_per_task.columns = ['Respondent ID', 'Question', 'Number of Alternatives']\n\nprint(\"\")\nprint(f\"Number of Respondents: {num_respondents}\")\nprint(f\"Number of Tasks per Respondent: {num_tasks}\")\nprint(f\"Number of Alternatives per Task: {alternatives_per_task['Number of Alternatives'].unique()}\")\n\n   resp.id  ques  alt carpool  seat cargo  eng  price  choice\n0        1     1    1     yes     6   2ft  gas     35       0\n1        1     1    2     yes     8   3ft  hyb     30       0\n2        1     1    3     yes     6   3ft  gas     30       1\n3        1     2    1     yes     6   2ft  gas     30       0\n4        1     2    2     yes     7   3ft  gas     35       1\n\nNumber of Respondents: 200\nNumber of Tasks per Respondent: 15\nNumber of Alternatives per Task: [3]\n\n\n\n\n\n\n\n   resp.id  ques  alt carpool  seat cargo  eng  price  choice\n0        1     1    1     yes     6   2ft  gas     35       0\n1        1     1    2     yes     8   3ft  hyb     30       0\n2        1     1    3     yes     6   3ft  gas     30       1\n3        1     2    1     yes     6   2ft  gas     30       0\n4        1     2    2     yes     7   3ft  gas     35       1\n\nNumber of Respondents: 200\nNumber of Tasks per Respondent: 15\nNumber of Alternatives per Task: [3]\n The attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\n\n\n\nModel\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\nfrom statsmodels.discrete.discrete_model import Logit\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import LabelEncoder\n\nlabelencoder = LabelEncoder()\nfor col in df_conjoint.columns:\n    if df_conjoint[col].dtype == object:\n        df_conjoint[col] = labelencoder.fit_transform(df_conjoint[col])\n\nX = df_conjoint.drop('choice', axis=1)\ny = df_conjoint['choice']\n\nmodel = Logit(y, X)\nresult = model.fit()\n\nprint(result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.603397\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 choice   No. Observations:                 9000\nModel:                          Logit   Df Residuals:                     8992\nMethod:                           MLE   Df Model:                            7\nDate:                Tue, 28 May 2024   Pseudo R-squ.:                 0.05203\nTime:                        15:16:21   Log-Likelihood:                -5430.6\nconverged:                       True   LL-Null:                       -5728.6\nCovariance Type:            nonrobust   LLR p-value:                1.673e-124\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nresp.id        0.0009      0.000      2.191      0.028    9.25e-05       0.002\nques           0.0122      0.005      2.299      0.021       0.002       0.023\nalt            0.1241      0.028      4.457      0.000       0.070       0.179\ncarpool        0.0327      0.051      0.642      0.521      -0.067       0.133\nseat           0.1805      0.021      8.632      0.000       0.140       0.221\ncargo          0.4571      0.046      9.872      0.000       0.366       0.548\neng            0.3216      0.028     11.299      0.000       0.266       0.377\nprice         -0.0864      0.004    -19.709      0.000      -0.095      -0.078\n==============================================================================\n\n\n\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 0.603397\n         Iterations 5\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                 choice   No. Observations:                 9000\nModel:                          Logit   Df Residuals:                     8992\nMethod:                           MLE   Df Model:                            7\nDate:                Tue, 28 May 2024   Pseudo R-squ.:                 0.05203\nTime:                        15:16:21   Log-Likelihood:                -5430.6\nconverged:                       True   LL-Null:                       -5728.6\nCovariance Type:            nonrobust   LLR p-value:                1.673e-124\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nresp.id        0.0009      0.000      2.191      0.028    9.25e-05       0.002\nques           0.0122      0.005      2.299      0.021       0.002       0.023\nalt            0.1241      0.028      4.457      0.000       0.070       0.179\ncarpool        0.0327      0.051      0.642      0.521      -0.067       0.133\nseat           0.1805      0.021      8.632      0.000       0.140       0.221\ncargo          0.4571      0.046      9.872      0.000       0.366       0.548\neng            0.3216      0.028     11.299      0.000       0.266       0.377\nprice         -0.0864      0.004    -19.709      0.000      -0.095      -0.078\n==============================================================================\n\nThe results show that the number of seats, cargo space, and engine type are all significant predictors of the minivan choice. The price coefficient is negative, indicating that higher prices reduce the probability of choosing a minivan. Ability to carry 3ft of cargo space is the most preferred feature, followed by a hybrid engine, and 7 seats.\n\n\n\n\nDollar Value of 3ft Cargo Space\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport statsmodels.formula.api as smf\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom scipy.stats import ttest_ind\nimport numpy as np\nfrom scipy.stats import norm\n\ndollar_value = (coef_cargo_3ft / -coef_price) *(3-2)\nprint(f\"The dollar value of 3ft of cargo space as compared to 2ft of cargo space is ${dollar_value:.2f}\")\n\nThe dollar value of 3ft of cargo space as compared to 2ft of cargo space is $5.29\n\n\n\n\n\n\n\nThe dollar value of 3ft of cargo space as compared to 2ft of cargo space is $5.29\n\n\n\n\nMarket Shares\n\n\n\n\n\nCode\n\n\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.api import Logit\n\nmarket = pd.DataFrame({\n    'Minivan': ['A', 'B', 'C', 'D', 'E', 'F'],\n    'seat_7': [1, 0, 0, 1, 0, 1],\n    'seat_8': [0, 0, 1, 0, 0, 0],\n    'cargo_3ft': [0, 0, 0, 1, 0, 0],\n    'eng_gas': [0, 1, 1, 1, 0, 0],\n    'eng_hyb': [1, 0, 0, 0, 0, 1],\n    'eng_elec': [0, 0, 0, 0, 1, 0],\n    'price': [30, 30, 30, 40, 40, 35]\n})\n\ncoefficients = pd.Series([0.1805, 0.1805, 0.4571, 0.3216, 0.3216, 0.3216, -0.0864], index=['seat_7', 'seat_8', 'cargo_3ft', 'eng_gas', 'eng_hyb', 'eng_elec', 'price'])\n\nmarket['Utility'] = np.dot(market[['seat_7', 'seat_8', 'cargo_3ft', 'eng_gas', 'eng_hyb', 'eng_elec', 'price']], coefficients)\n\nmarket['Probability'] = np.exp(market['Utility']) / np.sum(np.exp(market['Utility']))\n\nmarket['Market Share'] = market['Probability']\n\nprint(market.assign(Market_Share_Percent = [f'{x * 100:.2f}%' for x in market['Market Share']])[['Minivan', 'Market_Share_Percent']].to_string(index=False))\n\nMinivan Market_Share_Percent\n      A               22.21%\n      B               18.55%\n      C               22.21%\n      D               14.79%\n      E                7.82%\n      F               14.42%\n\n\n\n\n\n\n\nMinivan Market_Share_Percent\n      A               22.21%\n      B               18.55%\n      C               22.21%\n      D               14.79%\n      E                7.82%\n      F               14.42%"
  }
]